{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af10a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoom-private\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yoomin/ASR-w-GPT/wandb/run-20220511_234719-p64gugg3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/p64gugg3\" target=\"_blank\">vibrant-spaceship-536</a></strong> to <a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/p64gugg3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdc9848b280>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import math\n",
    "from itertools import groupby\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,0\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "cache_dir = \"/data4/yoomcache\"\n",
    "model_cache_dir = os.path.join(cache_dir, 'huggingface')\n",
    "data_cache_dir = os.path.join(cache_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(cache_dir, 'checkpoint')\n",
    "\n",
    "seed = 0\n",
    "random.seed(0)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "wandb.init(project=\"testing-wav2vec2gpt\", entity=\"yoom-private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e928a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "from wav2vec2GPTwCTC import *\n",
    "from configuration_wav2vec2gpt import Wav2Vec2GPTConfig\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor, GPT2Model\n",
    "from transformers import GPT2Tokenizer, AddedToken\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bffc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_pretrained = \"facebook/wav2vec2-base\"\n",
    "gpt_pretrained = \"gpt2\"\n",
    "\n",
    "# Should aware that pad_token_id is used to compute CTC loss, \n",
    "# so pad_token configuration for both tokenizer and model should be the same\n",
    "args = {\n",
    "    'pad_token': 'Ġ', 'pad_token_id': 220,\n",
    "    'unk_token': 'Ġ', 'unk_token_id': 220,\n",
    "#     'pad_token': \"<|endoftext|>\", 'pad_token_id': 50256,\n",
    "#     'unk_token': \"<|endoftext|>\", 'unk_token_id': 50256,\n",
    "    'bos_token': \"<|endoftext|>\", 'bos_token_id': 50256,\n",
    "    'eos_token': \"<|endoftext|>\", 'eos_token_id': 50256,\n",
    "    \n",
    "    'n_positions': 64, # VCTK: 42, \n",
    "    \n",
    "    'add_adapter': True,\n",
    "    'output_hidden_size': 128,\n",
    "    'num_adapter_layers': 3,\n",
    "    'adapter_kernel_size': [4, 4, 4, 0], \n",
    "    'adapter_stride':      [2, 2, 1, 1],\n",
    "    'adapter_padding':     [2, 2, 0, 0],\n",
    "    'adapter_bias': False\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "config = Wav2Vec2GPTConfig(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3728fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(wav2vec_pretrained, \n",
    "                                                             cache_dir=model_cache_dir,\n",
    "                                                             **args)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_pretrained,\n",
    "                                          cache_dir=model_cache_dir,\n",
    "                                          **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a15f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f821a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 881/881 [00:01<00:00, 492.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/data4/TTS/VCTK-Corpus/dataset-vctk-16k.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "del dataset['page'], dataset['index']\n",
    "# del dataset['audio_path']\n",
    "\n",
    "\n",
    "\n",
    "# dataset_size = len(dataset['text'])\n",
    "dataset_size = int(len(dataset['text']) * 0.02)\n",
    "\n",
    "max_audio_length = 0\n",
    "for arr in dataset['audio_array']:\n",
    "    if len(arr) > max_audio_length:\n",
    "        max_audio_length = len(arr)\n",
    "print(max_audio_length)\n",
    "\n",
    "\n",
    "for idx in tqdm(range(dataset_size)):\n",
    "    dataset['audio_array'][idx] = feature_extractor(dataset['audio_array'][idx], \n",
    "                                                    sampling_rate=dataset['sample_rate'],\n",
    "                                                    return_tensors=\"pt\",\n",
    "                                                    padding='max_length',\n",
    "                                                    max_length=max_audio_length\n",
    "                                                    ).input_values[0]\n",
    "del dataset['audio_array'][dataset_size:]\n",
    "print(len(dataset['audio_array']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4323698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([881, 64])\n"
     ]
    }
   ],
   "source": [
    "dataset['text'] = tokenizer(dataset['text'][:dataset_size],\n",
    "                            return_tensors=\"pt\",\n",
    "                            # padding='longest', # VCTK: 42,\n",
    "                            padding='max_length',\n",
    "                            max_length=args['n_positions']\n",
    "                            )\n",
    "print(dataset['text']['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c68c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "797d4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = (0.8, 0.9)\n",
    "indices = np.arange(dataset_size)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:int(dataset_size * split_ratio[0])]\n",
    "val_idx = indices[int(dataset_size * split_ratio[0]):int(dataset_size * split_ratio[1])]\n",
    "test_idx = indices[int(dataset_size * split_ratio[1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b33810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_values, tokenized_output, indices):\n",
    "        self.input_values = input_values\n",
    "        self.tokenized_output = tokenized_output\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_values'] = self.input_values[self.indices[idx]]\n",
    "        item['labels'] = self.tokenized_output['input_ids'][self.indices[idx]]\n",
    "        item['output_attention_mask'] = self.tokenized_output['attention_mask'][self.indices[idx]]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = CustomDataset(dataset['audio_array'], dataset['text'], train_idx)\n",
    "val_dataset = CustomDataset(dataset['audio_array'], dataset['text'], val_idx)\n",
    "test_dataset = CustomDataset(dataset['audio_array'], dataset['text'], test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fc3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f702adb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:356: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model2: ['wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'quantizer.weight_proj.bias', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'project_hid.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'project_q.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.feature_projection.layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.feature_projection.projection.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'quantizer.codevectors', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'quantizer.weight_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.feature_projection.projection.bias', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'project_q.bias', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.feature_projection.layer_norm.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'project_hid.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2GPTModel(\n",
      "  (wav2vec2): Wav2Vec2Model2(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2GroupNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (2): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (3): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (4): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (6): Wav2Vec2NoLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (adapter): Wav2Vec2Adapter(\n",
      "      (layers): ModuleList(\n",
      "        (0): Wav2Vec2AdapterLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(4,), stride=(2,), padding=(2,), bias=False)\n",
      "        )\n",
      "        (1): Wav2Vec2AdapterLayer(\n",
      "          (conv): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(2,), bias=False)\n",
      "        )\n",
      "        (2): Wav2Vec2AdapterLayer(\n",
      "          (conv): Conv1d(256, 512, kernel_size=(4,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (rnn_compressor): RNN(256, 768, batch_first=True)\n",
      "  (adaPool): AdaptiveMaxPool1d(output_size=64)\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(64, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2GPTModel(config=config)\n",
    "\n",
    "model.wav2vec2.from_pretrained(wav2vec_pretrained, cache_dir=model_cache_dir)\n",
    "# model_gpt = GPT2Model.from_pretrained(gpt_pretrained, cache_dir=model_cache_dir)\n",
    "# model.wte.weight = model_gpt.wte.weight # wte only\n",
    "# del model_gpt\n",
    "model.transformer.from_pretrained(gpt_pretrained, cache_dir=model_cache_dir)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# device_map = {\n",
    "#     0: [0, 1, 2, 3, 4,],\n",
    "#     1: [5, 6, 7, 8, 9, 10, 11, ],\n",
    "# }\n",
    "# model.parallelize(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1787c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()\n",
    "# model.freeze_feature_projection() # not exists here\n",
    "# model.freeze_wav2vec_encoder() # not exists here\n",
    "# model.freeze_wav2vec_adapter()\n",
    "# model.freeze_rnn_compressor()\n",
    "model.freeze_gpt_decoder()\n",
    "model.freeze_lm_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5408d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1392645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions.argmax(axis=-1)\n",
    "    del pred\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    \n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24040a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size / 2)\n",
    "\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "    output_dir=os.path.join(checkpoint_dir, \"wav2vec2gpt/unfreeze-adapter-rnn\"),\n",
    "    # do_train=True,\n",
    "    # do_eval=False,\n",
    "    # do_predict=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0,\n",
    "    num_train_epochs=100,\n",
    "    max_steps=-1,\n",
    "    # lr_scheduler_type=\"cosine\", \n",
    "    # warmup_ratio=0.0, \n",
    "    \n",
    "    logging_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=int(steps_per_epoch / 2),\n",
    "    save_steps=int(steps_per_epoch * 1),\n",
    "    eval_steps=int(steps_per_epoch / 2),\n",
    "    warmup_steps=int(steps_per_epoch * 10),\n",
    "    save_total_limit=10,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a32ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 704\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:471: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cudnn/RNN.cpp:926.)\n",
      "  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1161' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1161/1500 28:06 < 08:13, 0.69 it/s, Epoch 77.33/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14.381700</td>\n",
       "      <td>13.863242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14.077000</td>\n",
       "      <td>13.231899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>13.228800</td>\n",
       "      <td>12.196973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>12.415300</td>\n",
       "      <td>10.902843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>11.072900</td>\n",
       "      <td>9.568058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>10.014600</td>\n",
       "      <td>8.328327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>8.765400</td>\n",
       "      <td>7.159576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>7.815800</td>\n",
       "      <td>6.083826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>6.767000</td>\n",
       "      <td>5.088922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>5.907100</td>\n",
       "      <td>4.180136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>4.905700</td>\n",
       "      <td>3.435251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>4.019800</td>\n",
       "      <td>2.828387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.211800</td>\n",
       "      <td>2.397730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.594800</td>\n",
       "      <td>2.185658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.216400</td>\n",
       "      <td>2.105382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>2.106300</td>\n",
       "      <td>2.047443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>2.017215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.952500</td>\n",
       "      <td>1.986636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.994700</td>\n",
       "      <td>1.949825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.876100</td>\n",
       "      <td>1.939362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.921300</td>\n",
       "      <td>1.940549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.917000</td>\n",
       "      <td>1.902351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.934200</td>\n",
       "      <td>2.026893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.888000</td>\n",
       "      <td>1.866277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.854700</td>\n",
       "      <td>1.873453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.828600</td>\n",
       "      <td>1.859359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.875600</td>\n",
       "      <td>1.849763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>1.858343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.841600</td>\n",
       "      <td>1.838351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.837600</td>\n",
       "      <td>1.834858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.895000</td>\n",
       "      <td>1.828414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>1.803000</td>\n",
       "      <td>1.830340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>1.813600</td>\n",
       "      <td>1.823902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.756500</td>\n",
       "      <td>1.828636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.872400</td>\n",
       "      <td>1.831601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.809000</td>\n",
       "      <td>1.817297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>1.816011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.870100</td>\n",
       "      <td>1.831696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>1.799700</td>\n",
       "      <td>1.836797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.814400</td>\n",
       "      <td>1.825646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.753800</td>\n",
       "      <td>1.821829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>1.796600</td>\n",
       "      <td>1.817875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>1.772500</td>\n",
       "      <td>1.812083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>1.862200</td>\n",
       "      <td>1.809977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.711800</td>\n",
       "      <td>1.826034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>1.754300</td>\n",
       "      <td>1.820875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>1.821300</td>\n",
       "      <td>1.807177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>1.809000</td>\n",
       "      <td>1.809122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>1.810300</td>\n",
       "      <td>1.814909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.675400</td>\n",
       "      <td>1.813867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>1.767500</td>\n",
       "      <td>1.812318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>1.745900</td>\n",
       "      <td>1.802096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>1.767000</td>\n",
       "      <td>1.810904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>1.809550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>1.751700</td>\n",
       "      <td>1.819614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>1.730900</td>\n",
       "      <td>1.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>1.757300</td>\n",
       "      <td>1.814454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>1.692000</td>\n",
       "      <td>1.813021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>1.731500</td>\n",
       "      <td>1.830681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.914300</td>\n",
       "      <td>1.822788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>1.731700</td>\n",
       "      <td>1.824239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>1.797100</td>\n",
       "      <td>1.805021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>1.678400</td>\n",
       "      <td>1.807457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>1.771200</td>\n",
       "      <td>1.819834</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>1.677500</td>\n",
       "      <td>1.794268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>1.703900</td>\n",
       "      <td>1.801332</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>1.719100</td>\n",
       "      <td>1.810076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>1.767700</td>\n",
       "      <td>1.812952</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>1.748800</td>\n",
       "      <td>1.815137</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.659200</td>\n",
       "      <td>1.812461</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>1.748800</td>\n",
       "      <td>1.811681</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>1.744200</td>\n",
       "      <td>1.811206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>1.719800</td>\n",
       "      <td>1.816949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>1.696700</td>\n",
       "      <td>1.806811</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.664700</td>\n",
       "      <td>1.810463</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>1.703600</td>\n",
       "      <td>1.816387</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>1.686900</td>\n",
       "      <td>1.810788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>1.715400</td>\n",
       "      <td>1.820954</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>1.665200</td>\n",
       "      <td>1.820658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.649700</td>\n",
       "      <td>1.821855</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>1.720600</td>\n",
       "      <td>1.816341</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>1.646400</td>\n",
       "      <td>1.805366</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>1.687400</td>\n",
       "      <td>1.821315</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>1.658900</td>\n",
       "      <td>1.823592</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>1.676300</td>\n",
       "      <td>1.824787</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>1.669000</td>\n",
       "      <td>1.820354</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>1.722900</td>\n",
       "      <td>1.817823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>1.635600</td>\n",
       "      <td>1.824277</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>1.674100</td>\n",
       "      <td>1.828851</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.649600</td>\n",
       "      <td>1.822098</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>1.621200</td>\n",
       "      <td>1.828446</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>1.727600</td>\n",
       "      <td>1.823892</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>1.655400</td>\n",
       "      <td>1.829358</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>1.702600</td>\n",
       "      <td>1.819535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>1.725000</td>\n",
       "      <td>1.824105</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>1.634100</td>\n",
       "      <td>1.823565</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>1.605500</td>\n",
       "      <td>1.828477</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>1.698800</td>\n",
       "      <td>1.825737</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>1.645000</td>\n",
       "      <td>1.816948</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.689700</td>\n",
       "      <td>1.837367</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>1.650400</td>\n",
       "      <td>1.830765</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>1.677700</td>\n",
       "      <td>1.823464</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>1.679700</td>\n",
       "      <td>1.820667</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>1.609100</td>\n",
       "      <td>1.838261</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>1.680300</td>\n",
       "      <td>1.824109</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>1.681900</td>\n",
       "      <td>1.833276</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>1.665600</td>\n",
       "      <td>1.828410</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>1.652600</td>\n",
       "      <td>1.819736</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>1.641600</td>\n",
       "      <td>1.824359</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.675600</td>\n",
       "      <td>1.838862</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>1.662500</td>\n",
       "      <td>1.817036</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>1.644500</td>\n",
       "      <td>1.832666</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>1.628400</td>\n",
       "      <td>1.827646</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>1.645300</td>\n",
       "      <td>1.817965</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>1.674200</td>\n",
       "      <td>1.835675</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>1.674400</td>\n",
       "      <td>1.832003</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>1.665600</td>\n",
       "      <td>1.830718</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>1.626200</td>\n",
       "      <td>1.818426</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>1.691600</td>\n",
       "      <td>1.832013</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.617900</td>\n",
       "      <td>1.820742</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>1.654300</td>\n",
       "      <td>1.833795</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>1.677700</td>\n",
       "      <td>1.835893</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>1.705500</td>\n",
       "      <td>1.823838</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>1.638800</td>\n",
       "      <td>1.833609</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.683200</td>\n",
       "      <td>1.822152</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>1.587800</td>\n",
       "      <td>1.832062</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>1.676800</td>\n",
       "      <td>1.846046</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>1.612900</td>\n",
       "      <td>1.828921</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>1.664200</td>\n",
       "      <td>1.809673</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.734100</td>\n",
       "      <td>1.835968</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>1.610100</td>\n",
       "      <td>1.836120</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>1.634700</td>\n",
       "      <td>1.828377</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>1.605600</td>\n",
       "      <td>1.840360</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>1.693600</td>\n",
       "      <td>1.812851</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>1.832219</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>1.591500</td>\n",
       "      <td>1.840706</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>1.800800</td>\n",
       "      <td>1.816212</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>1.594300</td>\n",
       "      <td>1.820476</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>1.669500</td>\n",
       "      <td>1.842638</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.715400</td>\n",
       "      <td>1.833952</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>1.642300</td>\n",
       "      <td>1.830935</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>1.614100</td>\n",
       "      <td>1.828889</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>1.664700</td>\n",
       "      <td>1.825588</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>1.633900</td>\n",
       "      <td>1.822905</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>1.695400</td>\n",
       "      <td>1.840700</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>1.557100</td>\n",
       "      <td>1.836492</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>1.678600</td>\n",
       "      <td>1.836700</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>1.607900</td>\n",
       "      <td>1.841125</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>1.617800</td>\n",
       "      <td>1.835878</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.709000</td>\n",
       "      <td>1.840987</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057</td>\n",
       "      <td>1.644800</td>\n",
       "      <td>1.819072</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>1.667400</td>\n",
       "      <td>1.837657</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>1.571300</td>\n",
       "      <td>1.827551</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1078</td>\n",
       "      <td>1.693200</td>\n",
       "      <td>1.832675</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>1.643900</td>\n",
       "      <td>1.835837</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>1.623300</td>\n",
       "      <td>1.846018</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>1.698800</td>\n",
       "      <td>1.828537</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>1.652600</td>\n",
       "      <td>1.840088</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1113</td>\n",
       "      <td>1.600900</td>\n",
       "      <td>1.836301</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.693900</td>\n",
       "      <td>1.842777</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1127</td>\n",
       "      <td>1.582200</td>\n",
       "      <td>1.832407</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>1.672800</td>\n",
       "      <td>1.820156</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1141</td>\n",
       "      <td>1.626600</td>\n",
       "      <td>1.852408</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>1.649600</td>\n",
       "      <td>1.840795</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>1.625600</td>\n",
       "      <td>1.832060</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-15\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-15/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-15/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-330] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:471: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cudnn/RNN.cpp:926.)\n",
      "  result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-30\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-30/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-345] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-45\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-45/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-45/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-360] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-60\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-60/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-375] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-75\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-75/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-75/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-390] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-90\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-90/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-405] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-105\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-105/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-105/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-420] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-120\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-120/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-435] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-135\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-135/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-135/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-450] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-150\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-150/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-465] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-165\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-165/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-165/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-15] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-180\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-180/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-180/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-30] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-195\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-195/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-195/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-45] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-210\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-210/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-210/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-60] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-225\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-225/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-225/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-75] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-240\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-240/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-90] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-255\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-255/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-255/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-105] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-270\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-270/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-270/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-120] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-285\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-285/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-285/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-135] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-300\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-300/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-150] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-315\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-315/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-315/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-165] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-330\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-330/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-330/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-180] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-345\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-345/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-345/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-195] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-360\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-360/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-360/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-210] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-375\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-375/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-375/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-225] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-390\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-390/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-390/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-240] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-405\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-405/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-405/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-255] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-420\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-420/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-420/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-270] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-435\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-435/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-435/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-285] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-450\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-450/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-450/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-300] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-465\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-465/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-465/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-315] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-480\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-480/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-330] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-495\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-495/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-495/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-345] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-510\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-510/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-510/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-360] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-525\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-525/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-525/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-375] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-540\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-540/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-540/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-390] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-555\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-555/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-555/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-405] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-570\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-570/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-570/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-420] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-585\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-585/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-585/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-435] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-600\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-600/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-450] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-615\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-615/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-615/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-465] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-630\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-630/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-630/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-480] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-645\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-645/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-645/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-495] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-660\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-660/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-660/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-510] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-675\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-675/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-675/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-525] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-690\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-690/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-690/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-540] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-705\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-705/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-705/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-555] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-720\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-720/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-720/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-570] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-735\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-735/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-735/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-585] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-750\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-750/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-750/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-600] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-765\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-765/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-765/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-615] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-780\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-780/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-780/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-630] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-795\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-795/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-795/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-645] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-810\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-810/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-810/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-660] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-825\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-825/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-825/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-675] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-840\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-840/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-840/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-690] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-855\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-855/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-855/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-705] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-870\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-870/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-870/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-720] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-885\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-885/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-885/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-735] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-900\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-900/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-750] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-915\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-915/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-915/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-765] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-930\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-930/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-930/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-780] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-945\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-945/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-945/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-795] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-960\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-960/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-960/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-810] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-975\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-975/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-975/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-825] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-990\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-990/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-990/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-840] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1005\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1005/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1005/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-855] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1020\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1020/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1020/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-870] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1035\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1035/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1035/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-885] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1050\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1050/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1050/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-900] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1065\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1065/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1065/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-915] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1080\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1080/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1080/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-930] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1095/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1095/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-945] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1110\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1110/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1110/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-960] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1125\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1125/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1125/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-975] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1140\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1140/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1140/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-990] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1155\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1155/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1155/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn/checkpoint-1005] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# start training\n",
    "for i in range(3):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137abb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad51f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8814/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc42f47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### example\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "i = 4\n",
    "device = 'cuda:0'\n",
    "batch_idx = train_idx[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "audio_feature_batch = list()\n",
    "for idx in batch_idx:\n",
    "    audio_feature_batch.append(dataset['audio_array'][idx])\n",
    "audio_feature_batch = torch.stack(audio_feature_batch)\n",
    "print(audio_feature_batch.size())\n",
    "\n",
    "label_batch = dataset['text']['input_ids'][batch_idx]\n",
    "attention_batch = dataset['text']['attention_mask'][batch_idx]\n",
    "\n",
    "print(label_batch.size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_embedding = model(input_values=audio_feature_batch.to(device), \n",
    "                            labels=label_batch.to(device),\n",
    "                            output_attention_mask=attention_batch.to(device),)\n",
    "print(audio_embedding.logits.shape)\n",
    "\n",
    "pred_ids = torch.argmax(audio_embedding.logits, axis=-1)\n",
    "print(pred_ids.size())\n",
    "print()\n",
    "\n",
    "for idx in range(BATCH_SIZE):\n",
    "    print(tokenizer.decode(label_batch[idx]))\n",
    "    print(tokenizer.decode(pred_ids[idx]))\n",
    "    print()\n",
    "\n",
    "\n",
    "# for idx in range(BATCH_SIZE):\n",
    "#     print(tokenizer.decode([key for key, _group in groupby(label_batch[idx])]))\n",
    "#     print(tokenizer.decode([key for key, _group in groupby(pred_ids[idx])]))\n",
    "#     print(tokenizer.decode([key for key, _group in groupby(pred_ids[idx])]).replace('<|endoftext|>',''))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d47eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     print(model.wav2vec2(input_values=audio_feature_batch.to(device),)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba29a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdc490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3adb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     print(model.wav2vec2(audio_feature_batch)[0].shape)\n",
    "#     print(model.wav2vec2(audio_feature_batch.to(device))[0])\n",
    "    sns.heatmap(model.wav2vec2(audio_feature_batch.to(device))[0].cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.Audio(dataset['audio_path'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb1e4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "963acf10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e671b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.h[0].attn.c_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bcbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
