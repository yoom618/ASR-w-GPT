{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af10a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoom-private\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yoomin/ASR-w-GPT/wandb/run-20220423_003722-2wgq4bpu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/2wgq4bpu\" target=\"_blank\">fresh-voice-212</a></strong> to <a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/2wgq4bpu?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd5939d65b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import math\n",
    "from itertools import groupby\n",
    "\n",
    "import wandb\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# device = 'cuda:0'\n",
    "device = 'cpu'\n",
    "\n",
    "cache_dir = \"/data4/yoomcache\"\n",
    "model_cache_dir = os.path.join(cache_dir, 'huggingface')\n",
    "data_cache_dir = os.path.join(cache_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(cache_dir, 'checkpoint')\n",
    "\n",
    "seed = 0\n",
    "random.seed(0)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "wandb.init(project=\"testing-wav2vec2gpt\", entity=\"yoom-private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e928a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "from wav2vec2GPTwCTC import *\n",
    "from configuration_wav2vec2gpt import Wav2Vec2GPTConfig\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import GPT2Tokenizer, AddedToken\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bffc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_pretrained = \"facebook/wav2vec2-base\"\n",
    "gpt_pretrained = \"gpt2\"\n",
    "\n",
    "# Should aware that pad_token_id is used to compute CTC loss, \n",
    "# so pad_token configuration for both tokenizer and model should be the same\n",
    "args = {\n",
    "#     'pad_token': 'Ġ', 'pad_token_id': 220,\n",
    "#     'unk_token': 'Ġ', 'unk_token_id': 220,\n",
    "    'pad_token': \"<|endoftext|>\", 'pad_token_id': 50256,\n",
    "    'unk_token': \"<|endoftext|>\", 'unk_token_id': 50256,\n",
    "    'bos_token': \"<|endoftext|>\", 'bos_token_id': 50256,\n",
    "    'eos_token': \"<|endoftext|>\", 'eos_token_id': 50256,\n",
    "    \n",
    "    'n_positions': 64, # VCTK: 42, \n",
    "    \n",
    "    'add_adapter': True,\n",
    "    'adapter_kernel_size': 6, \n",
    "    'adapter_stride': 2,\n",
    "    'num_adapter_layers': 3,\n",
    "}\n",
    "\n",
    "\n",
    "config = Wav2Vec2GPTConfig(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3728fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(wav2vec_pretrained, \n",
    "                                                             cache_dir=model_cache_dir,\n",
    "                                                             **args)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_pretrained,\n",
    "                                          cache_dir=model_cache_dir,\n",
    "                                          **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a15f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30fed468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('/data4/TTS/VCTK-Corpus/dataset-vctk-16k(preprocessed).pkl'):\n",
    "#     with open('/data4/TTS/VCTK-Corpus/dataset-vctk-16k.pkl', 'rb') as f:\n",
    "#         dataset = pickle.load(f)\n",
    "#     del dataset['page'], dataset['index'], dataset['audio_path']\n",
    "\n",
    "\n",
    "#     max_audio_length = 0\n",
    "#     for arr in dataset['audio_array']:\n",
    "#         if len(arr) > max_audio_length:\n",
    "#             max_audio_length = len(arr)\n",
    "#     print(max_audio_length)\n",
    "\n",
    "\n",
    "#     for idx in tqdm(range(len(dataset['audio_array']))):\n",
    "#         dataset['audio_array'][idx] = feature_extractor(dataset['audio_array'][idx], \n",
    "#                                                         sampling_rate=dataset['sample_rate'],\n",
    "#                                                         return_tensors=\"pt\",\n",
    "#                                                         padding='max_length',\n",
    "#                                                         max_length=max_audio_length\n",
    "#                                                         ).input_values[0]\n",
    "#     dataset['audio_array'] = torch.stack(dataset['audio_array'])\n",
    "#     print(dataset['audio_array'].shape)\n",
    "\n",
    "\n",
    "#     with open('/data4/TTS/VCTK-Corpus/dataset-vctk-16k(preprocessed).pkl', 'wb') as f:\n",
    "#         pickle.dump(dataset, f)\n",
    "        \n",
    "        \n",
    "# else:\n",
    "#     with open('/data4/TTS/VCTK-Corpus/dataset-vctk-16k(preprocessed).pkl', 'rb') as f:\n",
    "#         dataset = pickle.load(f)\n",
    "#     print(dataset['audio_array'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f821a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 44070/44070 [01:35<00:00, 460.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/data4/TTS/VCTK-Corpus/dataset-vctk-16k.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "del dataset['page'], dataset['index'], dataset['audio_path']\n",
    "\n",
    "\n",
    "max_audio_length = 0\n",
    "for arr in dataset['audio_array']:\n",
    "    if len(arr) > max_audio_length:\n",
    "        max_audio_length = len(arr)\n",
    "print(max_audio_length)\n",
    "\n",
    "\n",
    "for idx in tqdm(range(len(dataset['audio_array']))):\n",
    "    dataset['audio_array'][idx] = feature_extractor(dataset['audio_array'][idx], \n",
    "                                                    sampling_rate=dataset['sample_rate'],\n",
    "                                                    return_tensors=\"pt\",\n",
    "                                                    padding='max_length',\n",
    "                                                    max_length=max_audio_length\n",
    "                                                    ).input_values[0]\n",
    "print(len(dataset['audio_array']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4323698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44070, 64])\n"
     ]
    }
   ],
   "source": [
    "dataset['text'] = tokenizer(dataset['text'],\n",
    "                            return_tensors=\"pt\",\n",
    "                            # padding='longest', # VCTK: 42,\n",
    "                            padding='max_length',\n",
    "                            max_length=args['n_positions']\n",
    "                            )\n",
    "print(dataset['text']['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c68c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797d4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = (0.8, 0.9)\n",
    "dataset_size = dataset['text']['attention_mask'].shape[0]\n",
    "indices = np.arange(dataset_size)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx = indices[:int(dataset_size * split_ratio[0])]\n",
    "val_idx = indices[int(dataset_size * split_ratio[0]):int(dataset_size * split_ratio[1])]\n",
    "test_idx = indices[int(dataset_size * split_ratio[1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b33810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_values, tokenized_output, indices):\n",
    "        self.input_values = input_values\n",
    "        self.tokenized_output = tokenized_output\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_values'] = self.input_values[self.indices[idx]]\n",
    "        item['labels'] = self.tokenized_output['input_ids'][self.indices[idx]]\n",
    "        item['output_attention_mask'] = self.tokenized_output['attention_mask'][self.indices[idx]]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = CustomDataset(dataset['audio_array'], dataset['text'], train_idx)\n",
    "val_dataset = CustomDataset(dataset['audio_array'], dataset['text'], val_idx)\n",
    "test_dataset = CustomDataset(dataset['audio_array'], dataset['text'], test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fd8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fc3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1787c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:356: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model2: ['project_q.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'quantizer.weight_proj.weight', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'quantizer.weight_proj.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'project_hid.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'quantizer.codevectors', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'project_q.bias', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'project_hid.bias', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.attention.q_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2GPTModel(config=config)\n",
    "\n",
    "model.wav2vec2.from_pretrained(wav2vec_pretrained, cache_dir=model_cache_dir)\n",
    "model.transformer.from_pretrained(gpt_pretrained, cache_dir=model_cache_dir)\n",
    "\n",
    "\n",
    "# device_map = {\n",
    "#     0: [0, 1, 2, 3, 4,],\n",
    "#     2: [5, 6, 7, 8, 9, 10, 11, ],\n",
    "# }\n",
    "# model.gpt2lm.parallelize(device_map)\n",
    "\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "model.freeze_feature_projection()\n",
    "# model.freeze_wav2vec_encoder() # not exists here\n",
    "model.unfreeze_wav2vec_adapter()\n",
    "model.unfreeze_rnn_compressor()\n",
    "model.freeze_gpt_decoder()\n",
    "model.unfreeze_lm_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5408d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd5b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1392645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load rouge for validation\n",
    "# rouge = load_metric(\"rouge\")\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     # all unnecessary tokens are removed\n",
    "#     pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "#     label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "#     rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "#     return {\n",
    "#         \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "#         \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "#         \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24040a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size / 2)\n",
    "\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "    output_dir=os.path.join(checkpoint_dir, \"wav2vec2gpt/unfreeze-adapter-rnn-lm\"),\n",
    "    # do_train=True,\n",
    "    # do_eval=False,\n",
    "    # do_predict=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0,\n",
    "    num_train_epochs=100,\n",
    "    max_steps=-1,\n",
    "    # lr_scheduler_type='cosine', \n",
    "    # warmup_ratio=0.0, \n",
    "    \n",
    "    logging_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=1 * steps_per_epoch,\n",
    "    save_steps=1 * steps_per_epoch,\n",
    "    eval_steps=1 * steps_per_epoch,\n",
    "    warmup_steps=10 * steps_per_epoch,\n",
    "    save_total_limit=10,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199a32ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 35256\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 73500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:942: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/cudnn/RNN.cpp:926.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73500' max='73500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [73500/73500 26:25:03, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>119.485500</td>\n",
       "      <td>25.785419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>22.916100</td>\n",
       "      <td>19.734936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2205</td>\n",
       "      <td>19.300800</td>\n",
       "      <td>19.108522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>18.708800</td>\n",
       "      <td>18.710142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>18.375400</td>\n",
       "      <td>18.470869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>18.086200</td>\n",
       "      <td>17.991512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5145</td>\n",
       "      <td>17.811200</td>\n",
       "      <td>17.808498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>17.547500</td>\n",
       "      <td>17.551544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6615</td>\n",
       "      <td>17.205100</td>\n",
       "      <td>17.552288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>16.893000</td>\n",
       "      <td>17.225233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8085</td>\n",
       "      <td>16.623900</td>\n",
       "      <td>16.666992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8820</td>\n",
       "      <td>16.345800</td>\n",
       "      <td>16.297943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9555</td>\n",
       "      <td>16.076000</td>\n",
       "      <td>16.520561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10290</td>\n",
       "      <td>15.873600</td>\n",
       "      <td>15.650925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11025</td>\n",
       "      <td>15.421600</td>\n",
       "      <td>15.422552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11760</td>\n",
       "      <td>15.068900</td>\n",
       "      <td>15.352070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12495</td>\n",
       "      <td>14.749900</td>\n",
       "      <td>15.244543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13230</td>\n",
       "      <td>14.667900</td>\n",
       "      <td>14.967047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13965</td>\n",
       "      <td>14.378500</td>\n",
       "      <td>14.657790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>14.068600</td>\n",
       "      <td>14.611323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15435</td>\n",
       "      <td>13.801600</td>\n",
       "      <td>14.093660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16170</td>\n",
       "      <td>13.404300</td>\n",
       "      <td>14.139506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16905</td>\n",
       "      <td>13.217000</td>\n",
       "      <td>13.815907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17640</td>\n",
       "      <td>13.202500</td>\n",
       "      <td>13.879829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18375</td>\n",
       "      <td>12.749700</td>\n",
       "      <td>13.695326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19110</td>\n",
       "      <td>12.267100</td>\n",
       "      <td>13.575985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19845</td>\n",
       "      <td>11.695100</td>\n",
       "      <td>13.247527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20580</td>\n",
       "      <td>11.103300</td>\n",
       "      <td>13.008592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21315</td>\n",
       "      <td>10.520100</td>\n",
       "      <td>12.974232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22050</td>\n",
       "      <td>9.935900</td>\n",
       "      <td>12.995761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22785</td>\n",
       "      <td>9.449200</td>\n",
       "      <td>13.102103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23520</td>\n",
       "      <td>9.107300</td>\n",
       "      <td>13.246756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24255</td>\n",
       "      <td>8.586500</td>\n",
       "      <td>13.374151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24990</td>\n",
       "      <td>7.968400</td>\n",
       "      <td>13.545319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25725</td>\n",
       "      <td>7.794400</td>\n",
       "      <td>13.726336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26460</td>\n",
       "      <td>7.319700</td>\n",
       "      <td>13.952817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27195</td>\n",
       "      <td>6.728500</td>\n",
       "      <td>14.144540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27930</td>\n",
       "      <td>6.560500</td>\n",
       "      <td>14.309627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28665</td>\n",
       "      <td>6.112700</td>\n",
       "      <td>14.405496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>5.642500</td>\n",
       "      <td>14.834188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30135</td>\n",
       "      <td>5.606500</td>\n",
       "      <td>15.254398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30870</td>\n",
       "      <td>5.367700</td>\n",
       "      <td>15.632439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31605</td>\n",
       "      <td>5.189400</td>\n",
       "      <td>16.658398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32340</td>\n",
       "      <td>4.898200</td>\n",
       "      <td>16.319170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33075</td>\n",
       "      <td>4.899300</td>\n",
       "      <td>16.426464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33810</td>\n",
       "      <td>4.868900</td>\n",
       "      <td>15.794681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34545</td>\n",
       "      <td>4.700400</td>\n",
       "      <td>15.706510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35280</td>\n",
       "      <td>4.568500</td>\n",
       "      <td>15.744806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36015</td>\n",
       "      <td>4.480700</td>\n",
       "      <td>16.720341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36750</td>\n",
       "      <td>4.221000</td>\n",
       "      <td>17.478506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37485</td>\n",
       "      <td>4.320100</td>\n",
       "      <td>16.575802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38220</td>\n",
       "      <td>4.012200</td>\n",
       "      <td>17.216995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38955</td>\n",
       "      <td>3.970400</td>\n",
       "      <td>18.800415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39690</td>\n",
       "      <td>4.060200</td>\n",
       "      <td>17.164083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40425</td>\n",
       "      <td>3.953600</td>\n",
       "      <td>18.007832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41160</td>\n",
       "      <td>3.675300</td>\n",
       "      <td>17.008591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41895</td>\n",
       "      <td>3.869700</td>\n",
       "      <td>18.638474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42630</td>\n",
       "      <td>3.627400</td>\n",
       "      <td>17.035635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43365</td>\n",
       "      <td>3.926600</td>\n",
       "      <td>18.511362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44100</td>\n",
       "      <td>3.718400</td>\n",
       "      <td>18.239641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44835</td>\n",
       "      <td>3.694100</td>\n",
       "      <td>17.386883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45570</td>\n",
       "      <td>3.716900</td>\n",
       "      <td>17.421213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46305</td>\n",
       "      <td>3.378700</td>\n",
       "      <td>17.410856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47040</td>\n",
       "      <td>3.654900</td>\n",
       "      <td>17.264023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47775</td>\n",
       "      <td>3.404500</td>\n",
       "      <td>17.722406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48510</td>\n",
       "      <td>3.275600</td>\n",
       "      <td>17.050537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49245</td>\n",
       "      <td>3.258000</td>\n",
       "      <td>17.954201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49980</td>\n",
       "      <td>3.312000</td>\n",
       "      <td>17.923641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50715</td>\n",
       "      <td>3.538700</td>\n",
       "      <td>19.432117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51450</td>\n",
       "      <td>3.399500</td>\n",
       "      <td>18.600756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52185</td>\n",
       "      <td>3.452800</td>\n",
       "      <td>18.830975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52920</td>\n",
       "      <td>3.415800</td>\n",
       "      <td>19.257864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53655</td>\n",
       "      <td>3.435500</td>\n",
       "      <td>17.659163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54390</td>\n",
       "      <td>3.302900</td>\n",
       "      <td>18.186831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55125</td>\n",
       "      <td>3.353500</td>\n",
       "      <td>18.186148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55860</td>\n",
       "      <td>3.462400</td>\n",
       "      <td>18.021818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56595</td>\n",
       "      <td>3.485900</td>\n",
       "      <td>17.647074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57330</td>\n",
       "      <td>3.344700</td>\n",
       "      <td>17.862577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58065</td>\n",
       "      <td>3.221600</td>\n",
       "      <td>18.161432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58800</td>\n",
       "      <td>3.312400</td>\n",
       "      <td>18.222551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59535</td>\n",
       "      <td>3.383900</td>\n",
       "      <td>18.162495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60270</td>\n",
       "      <td>3.262800</td>\n",
       "      <td>17.719398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61005</td>\n",
       "      <td>3.223600</td>\n",
       "      <td>18.784306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61740</td>\n",
       "      <td>3.256500</td>\n",
       "      <td>17.714609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62475</td>\n",
       "      <td>3.312800</td>\n",
       "      <td>18.641550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63210</td>\n",
       "      <td>3.242200</td>\n",
       "      <td>18.243881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63945</td>\n",
       "      <td>3.317600</td>\n",
       "      <td>17.886593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64680</td>\n",
       "      <td>3.197600</td>\n",
       "      <td>17.880110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65415</td>\n",
       "      <td>3.236700</td>\n",
       "      <td>18.228098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66150</td>\n",
       "      <td>3.040800</td>\n",
       "      <td>18.213308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66885</td>\n",
       "      <td>3.112300</td>\n",
       "      <td>18.111492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67620</td>\n",
       "      <td>3.096500</td>\n",
       "      <td>18.377977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68355</td>\n",
       "      <td>2.959800</td>\n",
       "      <td>18.279184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69090</td>\n",
       "      <td>3.380400</td>\n",
       "      <td>18.027948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69825</td>\n",
       "      <td>3.130300</td>\n",
       "      <td>18.080490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70560</td>\n",
       "      <td>3.110400</td>\n",
       "      <td>18.034037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71295</td>\n",
       "      <td>3.159700</td>\n",
       "      <td>17.992764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72030</td>\n",
       "      <td>3.225800</td>\n",
       "      <td>18.085321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72765</td>\n",
       "      <td>2.986200</td>\n",
       "      <td>17.970037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>3.067300</td>\n",
       "      <td>18.034136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-735\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-735/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-735/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-96] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-1470\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-1470/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-1470/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-98] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2205\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2205/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2205/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-100] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2940\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2940/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2940/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2938] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-3675\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-3675/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-3675/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5876] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-4410\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-4410/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-4410/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8814] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5145\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5145/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5145/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11752] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5880\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5880/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5880/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-14690] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-6615\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-6615/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-6615/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-17628] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-7350\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-7350/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-7350/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-20566] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8085\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8085/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8085/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-735] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8820\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8820/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8820/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-1470] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-9555\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-9555/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-9555/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2205] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-10290\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-10290/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-10290/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-2940] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11025\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11025/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11025/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-3675] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11760\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11760/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11760/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-4410] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-12495\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-12495/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-12495/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5145] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13230\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13230/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13230/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-5880] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13965\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13965/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13965/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-6615] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-14700\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-14700/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-14700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-7350] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-15435\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-15435/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-15435/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8085] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16170\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16170/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16170/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-8820] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16905\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16905/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16905/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-9555] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-17640\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-17640/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-17640/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-10290] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-18375\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-18375/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-18375/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11025] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19110\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19110/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19110/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-11760] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19845\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19845/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19845/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-12495] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-20580\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-20580/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-20580/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13230] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-21315\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-21315/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-21315/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-13965] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22050\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22050/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22050/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-14700] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22785\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22785/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22785/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-15435] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-23520\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-23520/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-23520/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16170] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24255\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24255/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24255/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-16905] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24990\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24990/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24990/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-17640] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-25725\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-25725/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-25725/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-18375] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-26460\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-26460/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-26460/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19110] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27195\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27195/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27195/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-19845] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27930\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27930/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27930/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-20580] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-28665\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-28665/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-28665/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-21315] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-29400\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-29400/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-29400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22050] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30135\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30135/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30135/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-22785] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30870\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30870/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30870/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-23520] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-31605\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-31605/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-31605/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24255] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-32340\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-32340/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-32340/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-24990] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33075\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33075/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33075/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-25725] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33810\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33810/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33810/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-26460] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-34545\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-34545/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-34545/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27195] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-35280\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-35280/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-35280/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-27930] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36015\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36015/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36015/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-28665] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36750\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36750/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36750/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-29400] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-37485\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-37485/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-37485/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30135] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38220\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38220/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38220/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-30870] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38955\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38955/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38955/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-31605] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-39690\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-39690/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-39690/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-32340] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-40425\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-40425/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-40425/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33075] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41160\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41160/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41160/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-33810] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41895\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41895/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41895/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-34545] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-42630\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-42630/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-42630/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-35280] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-43365\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-43365/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-43365/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36015] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44100\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44100/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44100/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-36750] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44835\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44835/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44835/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-37485] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-45570\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-45570/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-45570/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38220] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-46305\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-46305/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-46305/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-38955] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47040\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47040/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47040/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-39690] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47775\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47775/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47775/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-40425] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-48510\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-48510/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-48510/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41160] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49245\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49245/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49245/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-41895] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49980\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49980/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49980/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-42630] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-50715\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-50715/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-50715/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-43365] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-51450\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-51450/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-51450/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44100] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52185\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52185/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52185/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-44835] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52920\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52920/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52920/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-45570] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-53655\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-53655/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-53655/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-46305] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-54390\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-54390/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-54390/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47040] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55125\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55125/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55125/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-47775] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55860\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55860/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55860/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-48510] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-56595\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-56595/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-56595/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49245] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-57330\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-57330/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-57330/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-49980] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58065\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58065/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58065/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-50715] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58800\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58800/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-51450] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-59535\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-59535/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-59535/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52185] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-60270\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-60270/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-60270/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-52920] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61005\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61005/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61005/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-53655] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61740\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61740/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61740/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-54390] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-62475\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-62475/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-62475/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55125] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63210\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63210/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63210/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-55860] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63945\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63945/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63945/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-56595] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-64680\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-64680/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-64680/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-57330] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-65415\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-65415/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-65415/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58065] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66150\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66150/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66150/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-58800] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66885\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66885/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66885/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-59535] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-67620\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-67620/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-67620/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-60270] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-68355\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-68355/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-68355/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61005] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-69090\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-69090/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-69090/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-61740] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-69825\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-69825/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-69825/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-62475] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-70560\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-70560/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-70560/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63210] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-71295\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-71295/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-71295/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-63945] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-72030\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-72030/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-72030/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-64680] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-72765\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-72765/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-72765/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-65415] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4407\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-73500\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-73500/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-73500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=73500, training_loss=8.568285219361181, metrics={'train_runtime': 95106.6696, 'train_samples_per_second': 37.07, 'train_steps_per_second': 0.773, 'total_flos': 7.469035469007829e+20, 'train_loss': 8.568285219361181, 'epoch': 100.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5137abb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vivid-resonance-211</strong>: <a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/qyovc8x4\" target=\"_blank\">https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/qyovc8x4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220423_002948-qyovc8x4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad51f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-66885  checkpoint-69090  checkpoint-71295  checkpoint-73500\r\n",
      "checkpoint-67620  checkpoint-69825  checkpoint-72030\r\n",
      "checkpoint-68355  checkpoint-70560  checkpoint-72765\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc42f47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 308533])\n",
      "torch.Size([16, 64])\n",
      "torch.Size([16, 64, 50257])\n",
      "torch.Size([16, 64])\n",
      "\n",
      "This is no reflection on Rangers.\n",
      "The<|endoftext|> reflection<|endoftext|> interest<|endoftext|>.<|endoftext|>\n",
      "The reflection interest.\n",
      "\n",
      "Today, we begin to answer that question.\n",
      "The<|endoftext|>, will details are<|endoftext|> the suggestion<|endoftext|>.\n",
      "The, will details are the suggestion.\n",
      "\n",
      "We think a lot of Allan McGregor.\n",
      "It<|endoftext|> think<|endoftext|> other not away<|endoftext|> me<|endoftext|>.\n",
      "It think other not away me.\n",
      "\n",
      "This gives a financial incentive to switch.\n",
      "He<|endoftext|> her from incentive<|endoftext|> switch<|endoftext|>.<|endoftext|>\n",
      "He her from incentive switch.\n",
      "\n",
      "Sounds like The Sixth Sense?\n",
      "I<|endoftext|> like the Sixth Sense<|endoftext|>.<|endoftext|>\n",
      "I like the Sixth Sense.\n",
      "\n",
      "They had four children together.\n",
      "It<|endoftext|> support for<|endoftext|> to in<|endoftext|> matter<|endoftext|>.\n",
      "It support for to in matter.\n",
      "\n",
      "It was clear.\n",
      "It<|endoftext|> clear<|endoftext|>.<|endoftext|>\n",
      "It clear.\n",
      "\n",
      "He is a sort of a mystery figure.\n",
      "The<|endoftext|>'m, he would<|endoftext|> problem<|endoftext|>.<|endoftext|>\n",
      "The'm, he would problem.\n",
      "\n",
      "I am pleased with the result, as it was a possible upset.\n",
      "It<|endoftext|> years is the, the and by in<|endoftext|> at's<|endoftext|>.<|endoftext|>\n",
      "It years is the, the and by in at's.\n",
      "\n",
      "They made such decisions in London.\n",
      "We<|endoftext|> are looks decisions<|endoftext|> London<|endoftext|>.\n",
      "We are looks decisions London.\n",
      "\n",
      "Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.\n",
      "The<|endoftext|> spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob<|endoftext|>.\n",
      "The spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.\n",
      "\n",
      "When we played them last year, we were beaten by the weather.\n",
      "The<|endoftext|> them were already<|endoftext|> another November<|endoftext|>.\n",
      "The them were already another November.\n",
      "\n",
      "He refused to name the survivors.\n",
      "It<|endoftext|> is<|endoftext|> survivors<|endoftext|>.\n",
      "It is survivors.\n",
      "\n",
      "But they will not be compelled to join.\n",
      "I<|endoftext|> is be<|endoftext|>.\n",
      "I is be.\n",
      "\n",
      "It was a surprising decision.\n",
      "He<|endoftext|> was have a<|endoftext|> Islam<|endoftext|>.<|endoftext|>\n",
      "He was have a Islam.\n",
      "\n",
      "The Project is far from that.\n",
      "I<|endoftext|> is for<|endoftext|> us<|endoftext|>\n",
      "I is for us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### example\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    '/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-66885/pytorch_model.bin'))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "i = 3\n",
    "batch_idx = test_idx[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "audio_feature_batch = list()\n",
    "for idx in batch_idx:\n",
    "    audio_feature_batch.append(dataset['audio_array'][idx])\n",
    "audio_feature_batch = torch.stack(audio_feature_batch)\n",
    "print(audio_feature_batch.size())\n",
    "\n",
    "label_batch = dataset['text']['input_ids'][batch_idx]\n",
    "attention_batch = dataset['text']['attention_mask'][batch_idx]\n",
    "\n",
    "print(label_batch.size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_embedding = model(input_values=audio_feature_batch.to(device), \n",
    "                            labels=label_batch.to(device),\n",
    "                            output_attention_mask=attention_batch.to(device),)\n",
    "print(audio_embedding.logits.shape)\n",
    "\n",
    "pred_ids = torch.argmax(audio_embedding.logits, axis=-1)\n",
    "print(pred_ids.size())\n",
    "print()\n",
    "\n",
    "for idx in range(BATCH_SIZE):\n",
    "    print(tokenizer.decode(label_batch[idx]).replace('<|endoftext|>',''))\n",
    "    print(tokenizer.decode([key for key, _group in groupby(pred_ids[idx])]))\n",
    "    print(tokenizer.decode([key for key, _group in groupby(pred_ids[idx])]).replace('<|endoftext|>',''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0f60dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is no reflection on Rangers.\n",
      "The<|endoftext|> reflection<|endoftext|> interest<|endoftext|>.<|endoftext|>\n",
      "The reflection interest.\n",
      "\n",
      "Today, we begin to answer that question.\n",
      "The<|endoftext|>, will details are<|endoftext|> the suggestion<|endoftext|>.\n",
      "The, will details are the suggestion.\n",
      "\n",
      "We think a lot of Allan McGregor.\n",
      "It<|endoftext|> think<|endoftext|> other not away<|endoftext|> me<|endoftext|>.\n",
      "It think other not away me.\n",
      "\n",
      "This gives a financial incentive to switch.\n",
      "He<|endoftext|> her from incentive<|endoftext|> switch<|endoftext|>.<|endoftext|>\n",
      "He her from incentive switch.\n",
      "\n",
      "Sounds like The Sixth Sense?\n",
      "I<|endoftext|> like the Sixth Sense<|endoftext|>.<|endoftext|>\n",
      "I like the Sixth Sense.\n",
      "\n",
      "They had four children together.\n",
      "It<|endoftext|> support for<|endoftext|> to in<|endoftext|> matter<|endoftext|>.\n",
      "It support for to in matter.\n",
      "\n",
      "It was clear.\n",
      "It<|endoftext|> clear<|endoftext|>.<|endoftext|>\n",
      "It clear.\n",
      "\n",
      "He is a sort of a mystery figure.\n",
      "The<|endoftext|>'m, he would<|endoftext|> problem<|endoftext|>.<|endoftext|>\n",
      "The'm, he would problem.\n",
      "\n",
      "I am pleased with the result, as it was a possible upset.\n",
      "It<|endoftext|> years is the, the and by in<|endoftext|> at's<|endoftext|>.<|endoftext|>\n",
      "It years is the, the and by in at's.\n",
      "\n",
      "They made such decisions in London.\n",
      "We<|endoftext|> are looks decisions<|endoftext|> London<|endoftext|>.\n",
      "We are looks decisions London.\n",
      "\n",
      "Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.\n",
      "The<|endoftext|> spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob<|endoftext|>.\n",
      "The spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.\n",
      "\n",
      "When we played them last year, we were beaten by the weather.\n",
      "The<|endoftext|> them were already<|endoftext|> another November<|endoftext|>.\n",
      "The them were already another November.\n",
      "\n",
      "He refused to name the survivors.\n",
      "It<|endoftext|> is<|endoftext|> survivors<|endoftext|>.\n",
      "It is survivors.\n",
      "\n",
      "But they will not be compelled to join.\n",
      "I<|endoftext|> is be<|endoftext|>.\n",
      "I is be.\n",
      "\n",
      "It was a surprising decision.\n",
      "He<|endoftext|> was have a<|endoftext|> Islam<|endoftext|>.<|endoftext|>\n",
      "He was have a Islam.\n",
      "\n",
      "The Project is far from that.\n",
      "I<|endoftext|> is for<|endoftext|> us<|endoftext|>\n",
      "I is for us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### example\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    '/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-adapter-rnn-lm/checkpoint-72765/pytorch_model.bin'))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_embedding = model(input_values=audio_feature_batch.to(device), \n",
    "                            labels=label_batch.to(device),\n",
    "                            output_attention_mask=attention_batch.to(device),)\n",
    "\n",
    "for idx in range(BATCH_SIZE):\n",
    "    print(tokenizer.decode(label_batch[idx]).replace('<|endoftext|>',''))\n",
    "    print(tokenizer.decode([key for key, _group in groupby(pred_ids[idx])]))\n",
    "    print(tokenizer.decode([key for key, _group in groupby(pred_ids[idx])]).replace('<|endoftext|>',''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8be34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# IPython.display.Audio(dataset[4]['audio']['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb1e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661345c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
