{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af10a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoom-private\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yoomin/ASR-w-GPT/wandb/run-20220412_225108-1kkhj1b3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/1kkhj1b3\" target=\"_blank\">breezy-water-80</a></strong> to <a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/1kkhj1b3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1c0bb03160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = 'cuda:0'\n",
    "\n",
    "cache_dir = \"/data4/yoomcache\"\n",
    "model_cache_dir = os.path.join(cache_dir, 'huggingface')\n",
    "data_cache_dir = os.path.join(cache_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(cache_dir, 'checkpoint')\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "import math\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"testing-wav2vec2gpt\", entity=\"yoom-private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e928a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "from wav2vec2GPTwCTC import *\n",
    "from configuration_wav2vec2gpt import Wav2Vec2GPTConfig\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea767d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_pretrained = \"facebook/wav2vec2-base\"\n",
    "gpt_pretrained = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3728fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(wav2vec_pretrained, \n",
    "                                              cache_dir=model_cache_dir)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_pretrained,\n",
    "                                          cache_dir=model_cache_dir)\n",
    "tokenizer.bos_token = '<|endoftext|>'\n",
    "tokenizer.pad_token = 'Ġ'\n",
    "tokenizer.unk_token = 'Ġ'\n",
    "tokenizer.eos_token = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1787c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30fed468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/data4/yoomcache/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "Loading cached sorted indices for dataset at /data4/yoomcache/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b/cache-2f7c0cbee6ef3aa1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "     num_rows: 73\n",
       " }),\n",
       " 16000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", \n",
    "                       split=\"validation\", \n",
    "                       cache_dir=data_cache_dir\n",
    "                      )\n",
    "\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "dataset, sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4514bf9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_inputs = [d[\"audio\"][\"array\"] for d in dataset]\n",
    "len(audio_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa39e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_inputs = dataset[\"text\"]\n",
    "# text_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a694a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = [ # FROM `1272_128104.book.tsv`\n",
    "'Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.',\n",
    "\"Nor is Mr. Quilter's manner less interesting than his matter.\",\n",
    "\"He tells us that at this festive season of the year, with Christmas and roast beef looming before us, 'Similes drawn from eating and its results occur most readily to the mind.'\",\n",
    "\"He has grave doubts whether Sir Frederick Leighton's work is really 'Greek, after all,' and can discover in it but little of 'rocky Ithaca.'\",\n",
    "\"Linnell's pictures, are 'a sort of \\\"Up, Guards, and at 'em\\\" paintings,' and Mason's exquisite idylls are 'as national as a Jingo poem'! Mr. Birket Foster's landscapes 'smile at one much in the same way that Mr. Carker used to \\\"flash his teeth,\\\"' and Mr. John Collier gives his sitter 'a cheerful slap on the back, before he says, like a shampooer in a Turkish bath, \\\"Next man!\\\"\",\n",
    "\"It is obviously unnecessary for us to point out how luminous these criticisms are, how delicate in expression.\",\n",
    "\"On the general principles of art Mr. Quilter writes with equal lucidity.\",\n",
    "\"Painting, he tells us, is 'of a different quality to mathematics,' and finish in art is 'adding more fact'!\",\n",
    "\"As for etchings, they are of two kinds--British and foreign.\",\n",
    "\"He laments most bitterly the divorce that has been made between decorative art and 'what we usually call \\\"pictures,\\\"' makes the customary appeal to the Last Judgment, and reminds us that in the great days of art Michael Angelo was the 'furnishing upholsterer.'\",\n",
    "\"near the fire, and the ornaments Fred brought home from India on the mantel-board'!\",\n",
    "\"In fact, he is quite severe on Mr. Ruskin for not recognising that 'a picture should denote the frailty of man,' and remarks with pleasing courtesy and felicitous grace that 'many phases of feeling . . . \",\n",
    "\"Only, unfortunately, his own work never does get good.\",\n",
    "\"Mr. Quilter has missed his chance; for he has failed even to make himself the Tupper of Painting.\",\n",
    "\"By Harry Quilter, M.A.\",\n",
    "# FROM `1272_135031.book.tsv`\n",
    "\"\\\"Because you were sleeping instead of conquering, the lovely Rose Princess has become a fiddle without a bow, while poor Shaggy sits there a cooing dove!\\\"\",\n",
    "\"\\\"He has gone, and gone for good,\\\" answered Polychrome, who had managed to squeeze into the room beside the dragon and had witnessed the occurrences with much interest.\",\n",
    "'\"I have remained a prisoner only because I wished to be one,\" and with this he stepped forward and burst the stout chains as easily as if they had been threads.',\n",
    "\"The little girl had been asleep, but she heard the raps and opened the door.\",\n",
    "'\"The King has fled in disgrace and your friends are asking for you.\"',\n",
    "'\"I begged Ruggedo long ago to send him away, but he would not do so.\"',\n",
    "'\"I also offered to help your brother to escape, but he would not go.\"',\n",
    "'\"He eats and sleeps very steadily,\" replied the new King.',\n",
    "'\"I hope he doesn\\'t work too hard,\" said Shaggy.',\n",
    "'\"He doesn\\'t work at all.\"',\n",
    "\"In fact, there is nothing he can do in these dominions as well as our nomes, whose numbers are so great that it worries us to keep them all busy.\",\n",
    "'\"Not exactly,\" returned Kaliko.',\n",
    "'\"Where is my brother now?\"',\n",
    "'inquired Shaggy. \"In the Metal Forest.\"',\n",
    "'\"Where is that?\"',\n",
    "'\"The Metal Forest is in the Great Domed Cavern, the largest in all our dominions,\" replied Kaliko.',\n",
    "'Kaliko hesitated.',\n",
    "'\"However, if we look sharp, we may be able to discover one of these secret ways.\"',\n",
    "'\"Oh, no; I\\'m quite sure he didn\\'t.\"',\n",
    "'\"That\\'s funny,\" remarked Betsy thoughtfully.',\n",
    "'\"I don\\'t believe Ann knew any magic, or she\\'d have worked it before.\"',\n",
    "'\"I do not know,\" confessed Shaggy.',\n",
    "'\"True,\" agreed Kaliko.',\n",
    "'Kaliko went to the big gong and pounded on it just as Ruggedo used to do; but no one answered the summons.',\n",
    "\"Having returned to the royal cavern, Kaliko first pounded the gong and then sat in the throne, wearing Ruggedo's discarded ruby crown and holding in his hand the sceptre which Ruggedo had so often thrown at his head.\",\n",
    "# FROM `1272_141231.book.tsv`\n",
    "'_A man said to the universe: \"Sir, I exist!\"',\n",
    "\"Sweat covered Brion's body, trickling into the tight loincloth that was the only garment he wore.\",\n",
    "'The cut on his chest, still dripping blood, the ache of his overstrained eyes--even the soaring arena around him with the thousands of spectators--were trivialities not worth thinking about.',\n",
    "'His instant of panic was followed by a small sharp blow high on his chest.',\n",
    "'\"One minute,\" a voice said, and the time buzzer sounded.',\n",
    "'A minute is not a very large measure of time and his body needed every fraction of it.',\n",
    "\"The buzzer's whirr triggered his muscles into complete relaxation.\",\n",
    "'Only his heart and lungs worked on at a strong, measured rate.',\n",
    "'He was in reverie, sliding along the borders of consciousness.',\n",
    "'The contestants in the Twenties needed undisturbed rest, therefore nights in the dormitories were as quiet as death.',\n",
    "'Particularly so on this last night, when only two of the little cubicles were occupied, the thousands of others standing with dark, empty doors.',\n",
    "'The other voice snapped with a harsh urgency, clearly used to command.',\n",
    "'\"I\\'m here because the matter is of utmost importance, and Brandd is the one I must see. Now stand aside!\"',\n",
    "'\"The Twenties--\"',\n",
    "'He must have drawn his gun, because the intruder said quickly, \"Put that away. You\\'re being a fool!\"',\n",
    "'There was silence then and, still wondering, Brion was once more asleep.',\n",
    "'\"Ten seconds.\"',\n",
    "'he asked the handler who was kneading his aching muscles.',\n",
    "'A red-haired mountain of a man, with an apparently inexhaustible store of energy.',\n",
    "'There could be little art in this last and final round of fencing.',\n",
    "'Just thrust and parry, and victory to the stronger.',\n",
    "'Every man who entered the Twenties had his own training tricks.',\n",
    "'There appeared to be an immediate association with the death-trauma, as if the two were inextricably linked into one.',\n",
    "'The strength that enables someone in a trance to hold his body stiff and unsupported except at two points, the head and heels',\n",
    "'This is physically impossible when conscious.',\n",
    "'Others had died before during the Twenties, and death during the last round was in some ways easier than defeat.',\n",
    "'Breathing deeply, Brion softly spoke the auto-hypnotic phrases that triggered the process.',\n",
    "\"When the buzzer sounded he pulled his foil from his second's startled grasp, and ran forward.\",\n",
    "'Irolg looked amazed at the sudden fury of the attack--then smiled.',\n",
    "'He thought it was a last burst of energy, he knew how close they both were to exhaustion.',\n",
    "\"Brion saw something close to panic on his opponent's face when the man finally recognized his error.\",\n",
    "'A wave of despair rolled out from Irolg--Brion sensed it and knew the fifth point was his.',\n",
    "'Then the powerful twist that thrust it aside. In and under the guard.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f872cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b33810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_values, tokenized_output):\n",
    "        self.input_values = input_values\n",
    "        self.tokenized_output = tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_values'] = self.input_values['input_values'][idx]\n",
    "        item['labels'] = self.tokenized_output['input_ids'][idx]\n",
    "        item['output_attention_mask'] = self.tokenized_output['attention_mask'][idx]\n",
    "#         item['output_max_length'] = self.tokenized_output['input_ids'].shape[1]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_values['input_values'])\n",
    "\n",
    "    \n",
    "input_values = feature_extractor(audio_inputs, \n",
    "                                      sampling_rate=sampling_rate,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      padding='longest',\n",
    "                                     )\n",
    "\n",
    "tokenized_output = tokenizer(text_inputs, \n",
    "                          return_tensors=\"pt\",\n",
    "#                           padding='max_length',\n",
    "                          padding='longest',\n",
    "                         )\n",
    "\n",
    "train_dataset = CustomDataset(input_values, tokenized_output)\n",
    "# val_dataset = CustomDataset(input_values, tokenized_output)\n",
    "# test_dataset = CustomDataset(input_values, tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e489b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, audio_inputs, text_inputs):\n",
    "#         self.audio_inputs = audio_inputs\n",
    "#         self.text_inputs = text_inputs\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = dict()\n",
    "#         item['input_values'] = feature_extractor(self.audio_inputs[idx], \n",
    "#                                       sampling_rate=sampling_rate,\n",
    "#                                       return_tensors=\"pt\",\n",
    "#                                       padding='longest',\n",
    "#                                      )['input_values']\n",
    "#         tokenized_output = tokenizer(self.text_inputs[idx], \n",
    "#                                       return_tensors=\"pt\",\n",
    "# #                                       padding='max_length',\n",
    "#                                       padding='longest',\n",
    "#                                      )\n",
    "#         item['labels'] = tokenized_output['input_ids']\n",
    "#         item['output_attention_mask'] = tokenized_output['attention_mask']\n",
    "#         item['output_max_length'] = len(tokenized_output['input_ids'])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.audio_inputs)\n",
    "\n",
    "\n",
    "# train_dataset = CustomDataset(audio_inputs, text_inputs)\n",
    "# # val_dataset = CustomDataset(audio_inputs, text_inputs)\n",
    "# # test_dataset = CustomDataset(audio_inputs, text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f135b4ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:356: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model2: ['wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'project_hid.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'project_hid.weight', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'quantizer.codevectors', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'quantizer.weight_proj.weight', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'project_q.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'quantizer.weight_proj.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'project_q.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = Wav2Vec2GPTConfig()\n",
    "\n",
    "config.n_positions = tokenized_output['attention_mask'].shape[1]\n",
    "config.max_position_embeddings = config.n_positions\n",
    "\n",
    "# change configuration of adapter\n",
    "config.add_adapter = True\n",
    "config.adapter_kernel_size = 8\n",
    "config.adapter_stride = 2\n",
    "config.num_adapter_layers = 3\n",
    "\n",
    "\n",
    "model = Wav2Vec2GPTModel(config=config)\n",
    "\n",
    "model.wav2vec2.from_pretrained(wav2vec_pretrained, cache_dir=model_cache_dir)\n",
    "model.gpt2lm.from_pretrained(gpt_pretrained, cache_dir=model_cache_dir)\n",
    "\n",
    "\n",
    "# device_map = {\n",
    "#     0: [0, 1, 2, 3, 4,],\n",
    "#     2: [5, 6, 7, 8, 9, 10, 11, ],\n",
    "# }\n",
    "# model.gpt2lm.parallelize(device_map)\n",
    "\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "model.freeze_feature_projection()\n",
    "# model.freeze_wav2vec_encoder() # not exists here\n",
    "model.unfreeze_wav2vec_adapter()\n",
    "model.unfreeze_rnn_compressor()\n",
    "model.freeze_gpt_decoder()\n",
    "model.unfreeze_lm_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88b77ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6552e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2GPTModel(\n",
       "  (wav2vec2): Wav2Vec2Model2(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (2): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (3): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (4): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (6): Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (adapter): Wav2Vec2Adapter(\n",
       "      (layers): ModuleList(\n",
       "        (0): Wav2Vec2AdapterLayer(\n",
       "          (conv): Conv1d(768, 1536, kernel_size=(8,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (1): Wav2Vec2AdapterLayer(\n",
       "          (conv): Conv1d(768, 1536, kernel_size=(8,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (2): Wav2Vec2AdapterLayer(\n",
       "          (conv): Conv1d(768, 1536, kernel_size=(8,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (rnn_compressor): GRU(768, 769, batch_first=True)\n",
       "  (adaPool): AdaptiveMaxPool1d(output_size=107)\n",
       "  (gpt2lm): GPT2LMfromEmbedding(\n",
       "    (transformer): GPT2fromEmbedding(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(107, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f5e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1392645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = load_metric(\"rouge\")\n",
    "# rouge = load_metric(\"rouge\", experiment_id=1)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "    label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24040a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "    output_dir=os.path.join(checkpoint_dir, \"wav2vec2gpt/unfreeze-rnn\"),\n",
    "    # do_train=True,\n",
    "    # do_eval=True,\n",
    "#     do_predict=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size*5,\n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0,\n",
    "    num_train_epochs=100,\n",
    "    max_steps=-1,\n",
    "    # lr_scheduler_type='linear', warmup_ratio=0.0, \n",
    "    \n",
    "    logging_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=1 * steps_per_epoch,\n",
    "    save_steps=2 * steps_per_epoch,\n",
    "    eval_steps=1 * steps_per_epoch,\n",
    "    warmup_steps=10 * steps_per_epoch,\n",
    "    save_total_limit=0,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199a32ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 44:30, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>12.892700</td>\n",
       "      <td>18.833101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>9.358600</td>\n",
       "      <td>11.572621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>6.341200</td>\n",
       "      <td>7.226620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.809400</td>\n",
       "      <td>5.337698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.349200</td>\n",
       "      <td>4.468351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.283500</td>\n",
       "      <td>3.515601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.809900</td>\n",
       "      <td>3.151992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.665800</td>\n",
       "      <td>3.096469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.622600</td>\n",
       "      <td>2.973935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.579200</td>\n",
       "      <td>2.928264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.539100</td>\n",
       "      <td>2.870180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.511300</td>\n",
       "      <td>2.752360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.514000</td>\n",
       "      <td>2.650270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.494000</td>\n",
       "      <td>2.480768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.381500</td>\n",
       "      <td>2.272008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.266700</td>\n",
       "      <td>2.153058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.239200</td>\n",
       "      <td>1.821260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.101000</td>\n",
       "      <td>1.493572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.958900</td>\n",
       "      <td>1.226039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.020300</td>\n",
       "      <td>1.064256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.760600</td>\n",
       "      <td>0.839059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.811900</td>\n",
       "      <td>0.712968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>0.651695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.576700</td>\n",
       "      <td>0.543529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.634300</td>\n",
       "      <td>0.452510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.405545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.381482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.345106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.664100</td>\n",
       "      <td>0.336720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.312439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.321225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.290065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>0.288820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.292293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.269533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.265245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.259086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.250366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.549300</td>\n",
       "      <td>0.249035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.246293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.250162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.238273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.551900</td>\n",
       "      <td>0.241450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.237910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.494100</td>\n",
       "      <td>0.237565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.490400</td>\n",
       "      <td>0.237163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.564500</td>\n",
       "      <td>0.242617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.231868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.426700</td>\n",
       "      <td>0.221370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.213427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.381900</td>\n",
       "      <td>0.211342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.211469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>0.212238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.222409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.215296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.220694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.210372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.208122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.202343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.206930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.204848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.201187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.195598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.201501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.201894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.200895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>0.191306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.194103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.199255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.196258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.188628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.195410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.193303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>0.189374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.189521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.191885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.189662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.183069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>0.181419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.180928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.334100</td>\n",
       "      <td>0.180687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.178385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.364300</td>\n",
       "      <td>0.180982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.178821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.181152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.179146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.182648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.180243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.177054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.177093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.174516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>0.175466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.173926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.174143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.173453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.174119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-50\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-50/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-50/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-100\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-100/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-100/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-150\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-150/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-150/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-200\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-200/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-250\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-250/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-250/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-300\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-300/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-300/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-350\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-350/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-350/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-400\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-400/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-450\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-450/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-450/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-500\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-500/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-550\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-550/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-550/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-600\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-600/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-650\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-650/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-650/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-700\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-700/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-700/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-750\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-750/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-750/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-800\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-800/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-850\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-850/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-850/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-900\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-900/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-900/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-950/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-950/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1050\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1050/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1050/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1100\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1100/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1100/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1150\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1150/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1150/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1200\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1200/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1250\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1250/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1250/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1300\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1300/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1300/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1350\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1350/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1350/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1400\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1400/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1450\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1450/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1450/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1500\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1500/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1550\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1550/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1550/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1600\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1600/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1650\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1650/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1650/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1700\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1700/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1700/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1750\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1750/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1750/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1800\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1800/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1850\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1850/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1850/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1900\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1900/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1900/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1950\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1950/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1950/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2050\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2050/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2050/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2100\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2100/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2100/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2150\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2150/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2150/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2200\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2200/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2250\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2250/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2250/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2300\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2300/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2300/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2350\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2350/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2350/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2400\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2400/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2450\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2450/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2450/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2500\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2500/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-2500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.9027887992858886, metrics={'train_runtime': 2670.565, 'train_samples_per_second': 2.734, 'train_steps_per_second': 0.936, 'total_flos': 2.503700377073278e+18, 'train_loss': 0.9027887992858886, 'epoch': 100.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5137abb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▇▇▇█▇▇▇▇▇▇▅▁▇▇█▇▇██▇█▇▇█▅▁█▇▇▇▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/samples_per_second</td><td>▁▂▁▁▂▂▂▁▂▁▃█▁▁▁▁▁▁▁▁▁▁▁▁▃█▁▂▁▁▂▂▁▂▂▂▂▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▂▁▁▂▂▂▁▂▁▃█▁▁▁▁▁▁▁▁▁▁▁▁▃█▁▂▁▁▂▂▁▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.17412</td></tr><tr><td>eval/runtime</td><td>6.0089</td></tr><tr><td>eval/samples_per_second</td><td>12.149</td></tr><tr><td>eval/steps_per_second</td><td>0.832</td></tr><tr><td>train/epoch</td><td>100.0</td></tr><tr><td>train/global_step</td><td>2500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2271</td></tr><tr><td>train/total_flos</td><td>2.503700377073278e+18</td></tr><tr><td>train/train_loss</td><td>0.90279</td></tr><tr><td>train/train_runtime</td><td>2670.565</td></tr><tr><td>train/train_samples_per_second</td><td>2.734</td></tr><tr><td>train/train_steps_per_second</td><td>0.936</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">breezy-water-80</strong>: <a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/1kkhj1b3\" target=\"_blank\">https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/1kkhj1b3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220412_225108-1kkhj1b3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9b3b059",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 143920])\n",
      "torch.Size([8, 107])\n",
      "torch.Size([8, 107, 50257])\n",
      "torch.Size([8, 107])\n",
      "\n",
      "\"He doesn't work at all.\"\n",
      "\"\"HeHeHeHeHe doesn doesn doesn doesn't't work work work work at at at at at at.\" all.\".\".\"                                                                               \n",
      "\n",
      "In fact, there is nothing he can do in these dominions as well as our nomes, whose numbers are so great that it worries us to keep them all busy.\n",
      "InInIn fact fact fact fact fact,,, there is is is is is is nothing nothing he he he he can can can can do do do do do in in in in domin domin domin domin dominionsionsionsions as as as well well well well as as as as our our our n n nomesomesomesomesomes,,,,, numbers numbers numbers numbers numbers are are are so so so great great great great great that that that that it worries worries worries worries us us us to to to to keep keep\n",
      "\n",
      "\"Not exactly,\" returned Kaliko.\n",
      "\"\"\"\"NotNotNotNot exactly exactly exactly exactly returned returned returned returned Kal Kal Kal Kal Kal Kal Kalikoiko....                                                                              \n",
      "\n",
      "\"Where is my brother now?\"\n",
      "\"\"\"\"\"Where is is is my my my my brother brother brother brother now now now now now now?\"?\"                                                                                  \n",
      "\n",
      "inquired Shaggy. \"In the Metal Forest.\"\n",
      "inquinquirediredirediredired Sh Sh Sh Sh Shgygygygygygy......InInIn the the the the Metal Metal Metal Metal Forest Forest Forest Forest.\".\".\".\".\"                                                               \n",
      "\n",
      "\"Where is that?\"\n",
      "\"\"\"\"WhereWhereWhereWhereWhere is is that that that?\"?\"?\"?\"                                                                                         \n",
      "\n",
      "\"The Metal Forest is in the Great Domed Cavern, the largest in all our dominions,\" replied Kaliko.\n",
      "\"\"\"\"\"TheThe Metal Metal Metal Metal Metal Metal Forest is is is in in in in the the the the Great Great Great the Dom Dom Domed Cavern Cavern Cavern,,,,,,,, the the the largest largest in in in in in in all our our our domin domin domin dominionsionsions,\",\",\",\",\",\" replied replied Kal Kal Kalikoikoiko......                     \n",
      "\n",
      "Kaliko hesitated.\n",
      "KalKalKalKalKalikoiko hesitated hesitated hesitated hesitated...                                                                                             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "i = 3\n",
    "\n",
    "\n",
    "audio_batch = audio_inputs[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]\n",
    "audio_feature_batch = feature_extractor(audio_batch, \n",
    "                                      sampling_rate=sampling_rate,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      padding='longest',\n",
    "                                     ).input_values\n",
    "print(audio_feature_batch.size())\n",
    "\n",
    "\n",
    "text_batch = text_inputs[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "text_tokens_batch = tokenizer(text_batch, \n",
    "                              return_tensors=\"pt\",\n",
    "                              padding='max_length',\n",
    "                              max_length=train_dataset.tokenized_output['input_ids'].shape[1]\n",
    "                             )\n",
    "print(text_tokens_batch['attention_mask'].size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_embedding = model(input_values=audio_feature_batch.to(device), \n",
    "                            labels=text_tokens_batch['input_ids'].to(device),\n",
    "                            output_attention_mask=text_tokens_batch['attention_mask'].to(device),)\n",
    "print(audio_embedding.logits.shape)\n",
    "\n",
    "pred_ids = torch.argmax(audio_embedding.logits, axis=-1)\n",
    "print(pred_ids.size())\n",
    "print()\n",
    "\n",
    "for idx in range(BATCH_SIZE):\n",
    "    print(text_batch[idx])\n",
    "    print(tokenizer.decode(pred_ids[idx], output_word_offsets=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc286f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffaa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785f42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f83c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea63bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab67e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ff5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
