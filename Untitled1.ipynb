{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c717c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "epsilon = 1e-7\n",
    "mdim = 1000\n",
    "mdim2 = mdim*mdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0242f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"/data4/yoomcache\"\n",
    "model_cache_dir = os.path.join(cache_dir, 'huggingface')\n",
    "data_cache_dir = os.path.join(cache_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(cache_dir, 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2599dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4e1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b537b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2ForCTC(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "\n",
    "        if config.vocab_size is None:\n",
    "            raise ValueError(\n",
    "                f\"You are trying to instantiate {self.__class__} with a configuration that \"\n",
    "                \"does not define the vocabulary size of the language model head. Please \"\n",
    "                \"instantiate the model as follows: `Wav2Vec2ForCTC.from_pretrained(..., vocab_size=vocab_size)`. \"\n",
    "                \"or define `vocab_size` of your model's configuration.\"\n",
    "            )\n",
    "        output_hidden_size = (\n",
    "            config.output_hidden_size if hasattr(config, \"add_adapter\") and config.add_adapter else config.hidden_size\n",
    "        )\n",
    "        self.lm_head = nn.Linear(output_hidden_size, config.vocab_size)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        \"\"\"\n",
    "        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n",
    "        not be updated during training.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.\"\n",
    "            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "        self.freeze_feature_encoder()\n",
    "\n",
    "    def freeze_feature_encoder(self):\n",
    "        \"\"\"\n",
    "        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n",
    "        not be updated during training.\n",
    "        \"\"\"\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_values,\n",
    "        attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n",
    "            Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n",
    "            the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n",
    "            All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,\n",
    "            config.vocab_size - 1]`.\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "\n",
    "            if labels.max() >= self.config.vocab_size:\n",
    "                raise ValueError(f\"Label values must be <= vocab_size: {self.config.vocab_size}\")\n",
    "\n",
    "            # retrieve loss input_lengths from attention_mask\n",
    "            attention_mask = (\n",
    "                attention_mask if attention_mask is not None else torch.ones_like(input_values, dtype=torch.long)\n",
    "            )\n",
    "            input_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "\n",
    "            # assuming that padded tokens are filled with -100\n",
    "            # when not being attended to\n",
    "            labels_mask = labels >= 0\n",
    "            target_lengths = labels_mask.sum(-1)\n",
    "            flattened_targets = labels.masked_select(labels_mask)\n",
    "\n",
    "            # ctc_loss doesn't support fp16\n",
    "            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n",
    "\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                loss = nn.functional.ctc_loss(\n",
    "                    log_probs,\n",
    "                    flattened_targets,\n",
    "                    input_lengths,\n",
    "                    target_lengths,\n",
    "                    blank=self.config.pad_token_id,\n",
    "                    reduction=self.config.ctc_loss_reduction,\n",
    "                    zero_infinity=self.config.ctc_zero_infinity,\n",
    "                )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutput(\n",
    "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a403501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", \n",
    "                                       cache_dir=model_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd4d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
