{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af10a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoom-private\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yoomin/ASR-w-GPT/wandb/run-20220414_014231-1hvon2an</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/1hvon2an\" target=\"_blank\">rose-wood-131</a></strong> to <a href=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yoom-private/testing-wav2vec2gpt/runs/1hvon2an?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff1cde96130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "cache_dir = \"/data4/yoomcache\"\n",
    "model_cache_dir = os.path.join(cache_dir, 'huggingface')\n",
    "data_cache_dir = os.path.join(cache_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(cache_dir, 'checkpoint')\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "import math\n",
    "from itertools import groupby\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"testing-wav2vec2gpt\", entity=\"yoom-private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e928a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "from wav2vec2GPTwCTC import *\n",
    "from configuration_wav2vec2gpt import Wav2Vec2GPTConfig\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import GPT2Tokenizer, AddedToken\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bffc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_pretrained = \"facebook/wav2vec2-base\"\n",
    "gpt_pretrained = \"gpt2\"\n",
    "\n",
    "# Should aware that pad_token_id is used to compute CTC loss, \n",
    "# so pad_token configuration for both tokenizer and model should be the same\n",
    "args = {\n",
    "    'pad_token': 'Ġ', 'pad_token_id': 220,\n",
    "    'unk_token': 'Ġ', 'unk_token_id': 220,\n",
    "    # 'pad_token': \"<|endoftext|>\", 'pad_token_id': 50256,\n",
    "    # 'unk_token': \"<|endoftext|>\", 'unk_token_id': 50256,\n",
    "    'bos_token': \"<|endoftext|>\", 'bos_token_id': 50256,\n",
    "    'eos_token': \"<|endoftext|>\", 'eos_token_id': 50256,\n",
    "    \n",
    "    'n_positions': 128,\n",
    "    \n",
    "    'add_adapter': True,\n",
    "    'adapter_kernel_size': 3, \n",
    "    'adapter_stride': 2,\n",
    "    'num_adapter_layers': 3,\n",
    "}\n",
    "\n",
    "\n",
    "config = Wav2Vec2GPTConfig(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3728fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(wav2vec_pretrained, \n",
    "                                                             cache_dir=model_cache_dir,\n",
    "                                                             **args)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_pretrained,\n",
    "                                          cache_dir=model_cache_dir,\n",
    "                                          **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1787c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:356: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model2: ['wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'project_hid.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'quantizer.weight_proj.bias', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'project_q.weight', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'project_hid.bias', 'quantizer.weight_proj.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'quantizer.codevectors', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'project_q.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2GPTModel(config=config)\n",
    "\n",
    "model.wav2vec2.from_pretrained(wav2vec_pretrained, cache_dir=model_cache_dir)\n",
    "model.transformer.from_pretrained(gpt_pretrained, cache_dir=model_cache_dir)\n",
    "\n",
    "\n",
    "# device_map = {\n",
    "#     0: [0, 1, 2, 3, 4,],\n",
    "#     2: [5, 6, 7, 8, 9, 10, 11, ],\n",
    "# }\n",
    "# model.gpt2lm.parallelize(device_map)\n",
    "\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "model.freeze_feature_projection()\n",
    "# model.freeze_wav2vec_encoder() # not exists here\n",
    "model.unfreeze_wav2vec_adapter()\n",
    "model.unfreeze_rnn_compressor()\n",
    "model.freeze_gpt_decoder()\n",
    "model.unfreeze_lm_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705b1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae2e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fed468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/data4/yoomcache/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "Loading cached sorted indices for dataset at /data4/yoomcache/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b/cache-2f7c0cbee6ef3aa1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "    num_rows: 73\n",
      "}) 16000 73\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", \n",
    "                       split=\"validation\", \n",
    "                       cache_dir=data_cache_dir\n",
    "                      )\n",
    "\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "audio_inputs = [d[\"audio\"][\"array\"] for d in dataset]\n",
    "\n",
    "print(dataset, sampling_rate, len(audio_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa39e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_inputs = dataset[\"text\"]\n",
    "from example.librispeech_asr_demo import text_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdb133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b33810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_values, tokenized_output):\n",
    "        self.input_values = input_values\n",
    "        self.tokenized_output = tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        item['input_values'] = self.input_values['input_values'][idx]\n",
    "        item['labels'] = self.tokenized_output['input_ids'][idx]\n",
    "        item['output_attention_mask'] = self.tokenized_output['attention_mask'][idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_values['input_values'])\n",
    "\n",
    "    \n",
    "input_values = feature_extractor(audio_inputs, \n",
    "                                      sampling_rate=sampling_rate,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      padding='longest',\n",
    "                                     )\n",
    "\n",
    "tokenized_output = tokenizer(text_inputs,\n",
    "                             return_tensors=\"pt\",\n",
    "                             padding='longest',\n",
    "                         )\n",
    "\n",
    "train_dataset = CustomDataset(input_values, tokenized_output)\n",
    "# val_dataset = CustomDataset(input_values, tokenized_output)\n",
    "# test_dataset = CustomDataset(input_values, tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135b4ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f5e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1392645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load rouge for validation\n",
    "# rouge = load_metric(\"rouge\")\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     # all unnecessary tokens are removed\n",
    "#     pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "#     label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "#     rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "#     return {\n",
    "#         \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "#         \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "#         \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24040a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "#     predict_with_generate=True,\n",
    "    output_dir=os.path.join(checkpoint_dir, \"wav2vec2gpt/unfreeze-rnn\"),\n",
    "    # do_train=True,\n",
    "    # do_eval=False,\n",
    "    # do_predict=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size * 5,\n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0,\n",
    "    num_train_epochs=200,\n",
    "    max_steps=-1,\n",
    "    # lr_scheduler_type='linear', warmup_ratio=0.0, \n",
    "    \n",
    "    logging_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=1 * steps_per_epoch,\n",
    "    save_steps=2 * steps_per_epoch,\n",
    "    eval_steps=1 * steps_per_epoch,\n",
    "    warmup_steps=10 * steps_per_epoch,\n",
    "    save_total_limit=10,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a32ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1082' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1082/5000 06:07 < 22:13, 2.94 it/s, Epoch 43.24/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3778.227800</td>\n",
       "      <td>18605.064453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3702.246300</td>\n",
       "      <td>17605.548828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3548.174700</td>\n",
       "      <td>15933.613281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3340.755600</td>\n",
       "      <td>14062.597656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2993.713100</td>\n",
       "      <td>11564.162109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2479.292300</td>\n",
       "      <td>8303.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1736.343800</td>\n",
       "      <td>4069.257812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>800.646100</td>\n",
       "      <td>2689.722168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>499.210200</td>\n",
       "      <td>2068.931885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>517.000400</td>\n",
       "      <td>1997.456543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>434.675700</td>\n",
       "      <td>1883.126343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>411.396100</td>\n",
       "      <td>1833.438599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>389.926800</td>\n",
       "      <td>1798.477295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>415.299600</td>\n",
       "      <td>1773.957520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>534.265200</td>\n",
       "      <td>1774.732056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>368.872100</td>\n",
       "      <td>1715.383301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>346.874300</td>\n",
       "      <td>1662.868652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>424.263100</td>\n",
       "      <td>1606.301392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>339.646200</td>\n",
       "      <td>1578.290161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>345.939000</td>\n",
       "      <td>1593.336182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>326.710100</td>\n",
       "      <td>1546.870850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>356.837700</td>\n",
       "      <td>1515.157349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>396.754700</td>\n",
       "      <td>1494.158569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>324.172100</td>\n",
       "      <td>1503.301147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>315.008200</td>\n",
       "      <td>1446.936523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>334.266800</td>\n",
       "      <td>1409.588379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>311.709800</td>\n",
       "      <td>1396.320190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>284.317800</td>\n",
       "      <td>1370.065552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>371.255300</td>\n",
       "      <td>1370.752075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>296.636300</td>\n",
       "      <td>1319.529541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>315.669200</td>\n",
       "      <td>1321.534302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>308.634400</td>\n",
       "      <td>1289.850220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>320.626800</td>\n",
       "      <td>1315.557495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>297.047300</td>\n",
       "      <td>1293.085449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>306.950300</td>\n",
       "      <td>1301.420044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>282.605700</td>\n",
       "      <td>1273.332886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>277.210600</td>\n",
       "      <td>1255.182617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>280.827800</td>\n",
       "      <td>1246.193481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>269.082100</td>\n",
       "      <td>1271.141602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>258.414300</td>\n",
       "      <td>1285.276367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>277.280700</td>\n",
       "      <td>1272.825684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>302.506100</td>\n",
       "      <td>1210.629517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>274.900100</td>\n",
       "      <td>1189.015259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-50\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-50/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-100\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-100/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-150\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-150/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-200\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-200/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-250\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-250/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-250/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4750] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-300\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-300/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-350\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-350/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-350/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4850] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-400\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-400/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-450\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-450/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-450/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-4950] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-500\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-500/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-550\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-550/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-550/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-600\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-600/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-650\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-650/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-650/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-700\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-700/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-750/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-750/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-800\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-800/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-850\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-850/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-850/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-900\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-900/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-950\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-950/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-950/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1050\n",
      "Configuration saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1050/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-1050/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/wav2vec2gpt/unfreeze-rnn/checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 73\n",
      "  Batch size = 15\n"
     ]
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3b059",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "i = 3\n",
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "audio_batch = audio_inputs[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]\n",
    "audio_feature_batch = feature_extractor(audio_batch, \n",
    "                                      sampling_rate=sampling_rate,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      padding='longest',\n",
    "                                     ).input_values\n",
    "print(audio_feature_batch.size())\n",
    "\n",
    "\n",
    "text_batch = text_inputs[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "text_tokens_batch = tokenizer(text_batch, \n",
    "                              return_tensors=\"pt\",\n",
    "                              padding='max_length',\n",
    "                              max_length=train_dataset.tokenized_output['input_ids'].shape[1]\n",
    "                             )\n",
    "print(text_tokens_batch['attention_mask'].size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_embedding = model(input_values=audio_feature_batch.to(device), \n",
    "                            labels=text_tokens_batch['input_ids'].to(device),\n",
    "                            output_attention_mask=text_tokens_batch['attention_mask'].to(device),)\n",
    "print(audio_embedding.logits.shape)\n",
    "\n",
    "pred_ids = torch.argmax(audio_embedding.logits, axis=-1)\n",
    "print(pred_ids.size())\n",
    "print()\n",
    "\n",
    "for idx in range(BATCH_SIZE):\n",
    "    print(text_batch[idx])\n",
    "    print(tokenizer.decode([key for key, _group in groupby(pred_ids[idx])]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.Audio(dataset[listen_idx-6]['audio']['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ff5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089aa47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2a7a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 220\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32718c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
