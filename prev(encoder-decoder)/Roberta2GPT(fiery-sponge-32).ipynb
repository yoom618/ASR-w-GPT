{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39678a37",
   "metadata": {},
   "source": [
    "### 0. Initial Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0bf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install datasets==1.0.2\n",
    "# !pip install transformers==4.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44238cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoom618\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yoomin/wandb/run-20220401_175809-1rt47ylv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yoom618/testing-roberta2gpt/runs/1rt47ylv\" target=\"_blank\">fiery-sponge-32</a></strong> to <a href=\"https://wandb.ai/yoom618/testing-roberta2gpt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yoom618/testing-roberta2gpt/runs/1rt47ylv?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7febf1b5beb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "cache_dir = \"/data4/yoomcache\"\n",
    "model_cache_dir = os.path.join(cache_dir, 'huggingface')\n",
    "data_cache_dir = os.path.join(cache_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(cache_dir, 'checkpoint')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "from transformers import BertTokenizer, RobertaTokenizer, GPT2Tokenizer\n",
    "from transformers import AutoConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"testing-roberta2gpt\", entity=\"yoom618\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6e87b",
   "metadata": {},
   "source": [
    "### 1. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435b40c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_encoder = AutoConfig.from_pretrained(\"roberta-base\", cache_dir=model_cache_dir)\n",
    "config_decoder = AutoConfig.from_pretrained(\"gpt2\", cache_dir=model_cache_dir)\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder, cache_dir=model_cache_dir)\n",
    "model = EncoderDecoderModel(config=config)\n",
    "# model.save_pretrained(\"roberta2gpt\", cache_dir=model_cache_dir)\n",
    "# model = EncoderDecoderModel.from_pretrained(\"roberta2gpt\", cache_dir=model_cache_dir)\n",
    "\n",
    "model.encoder.encoder.layer = model.encoder.encoder.layer[:6]\n",
    "model.decoder.transformer.h = model.decoder.transformer.h[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe24aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", cache_dir=model_cache_dir)\n",
    "encoder_tokenizer.bos_token = encoder_tokenizer.cls_token  # CLS token will work as BOS token\n",
    "encoder_tokenizer.eos_token = encoder_tokenizer.sep_token  # SEP token will work as EOS token\n",
    "\n",
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "decoder_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=model_cache_dir)\n",
    "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.unk_token\n",
    "\n",
    "\n",
    "model.config.decoder_start_token_id = encoder_tokenizer.cls_token_id\n",
    "model.config.eos_token_id = encoder_tokenizer.sep_token_id\n",
    "model.config.pad_token_id = encoder_tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.max_length = 142\n",
    "model.config.min_length = 56\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze decoder parameters\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bce84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467b21e7",
   "metadata": {},
   "source": [
    "### 2. Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feaded22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):    # Tokenizer will automatically set [BOS] <text> [EOS] \n",
    "    encoder_length, decoder_length = 512, 128\n",
    "    inputs = encoder_tokenizer(batch[\"article\"], \n",
    "                               padding=\"max_length\", \n",
    "                               truncation=True, \n",
    "                               max_length=encoder_length)\n",
    "    outputs = decoder_tokenizer(batch[\"highlights\"], \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                max_length=decoder_length)\n",
    "    \n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "\n",
    "    # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "    batch[\"labels\"] = -100 if batch[\"decoder_attention_mask\"] == 0 else batch[\"labels\"]\n",
    "\n",
    "    assert len(inputs.input_ids) == encoder_length\n",
    "    assert len(outputs.input_ids) == decoder_length\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d77189b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(cache_dir, 'preprocessed/train')):\n",
    "    train_dataset = load_from_disk(os.path.join(cache_dir, 'preprocessed/train'))\n",
    "else:\n",
    "    train_dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\", split=\"train\", cache_dir=data_cache_dir)\n",
    "    train_dataset = train_dataset.map(\n",
    "        map_to_encoder_decoder_inputs, \n",
    "        # batched=True, \n",
    "        # batch_size=batch_size, \n",
    "        remove_columns=['id', 'article', 'highlights'],\n",
    "    )\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", \n",
    "        columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    "    )\n",
    "    \n",
    "    train_dataset.save_to_disk(os.path.join(cache_dir, 'preprocessed/train'))\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(cache_dir, 'preprocessed/val')):\n",
    "    val_dataset = load_from_disk(os.path.join(cache_dir, 'preprocessed/val'))\n",
    "else:\n",
    "    val_dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\", split=\"validation\", cache_dir=data_cache_dir)\n",
    "    val_dataset = val_dataset.map(\n",
    "        map_to_encoder_decoder_inputs, \n",
    "        # batched=True, \n",
    "        # batch_size=batch_size, \n",
    "        remove_columns=['id', 'article', 'highlights'],\n",
    "    )\n",
    "    val_dataset.set_format(\n",
    "        type=\"torch\", \n",
    "        columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    "    )\n",
    "    val_dataset.save_to_disk(os.path.join(cache_dir, 'preprocessed/val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80414621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63779bf8",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cde9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = load_metric(\"rouge\")\n",
    "# rouge = load_metric(\"rouge\", experiment_id=1)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "    label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1453320d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 28711\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 179500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='179500' max='179500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [179500/179500 58:27:58, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.520600</td>\n",
       "      <td>3.665480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.723100</td>\n",
       "      <td>3.675316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.717300</td>\n",
       "      <td>3.672190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.719200</td>\n",
       "      <td>3.659882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.713000</td>\n",
       "      <td>3.661578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.706600</td>\n",
       "      <td>3.668557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.545700</td>\n",
       "      <td>3.296617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.345100</td>\n",
       "      <td>3.272192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.234900</td>\n",
       "      <td>3.183884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.138000</td>\n",
       "      <td>3.083094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.069900</td>\n",
       "      <td>3.047034</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.021200</td>\n",
       "      <td>3.021716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.003900</td>\n",
       "      <td>2.994274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.974200</td>\n",
       "      <td>2.965233</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.915600</td>\n",
       "      <td>2.916864</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.873300</td>\n",
       "      <td>2.878583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.816400</td>\n",
       "      <td>2.814847</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.741600</td>\n",
       "      <td>2.770210</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.698200</td>\n",
       "      <td>2.752803</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>2.736599</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.641200</td>\n",
       "      <td>2.694124</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.604100</td>\n",
       "      <td>2.664863</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.554100</td>\n",
       "      <td>2.623903</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.517100</td>\n",
       "      <td>2.590804</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.493400</td>\n",
       "      <td>2.565449</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.464800</td>\n",
       "      <td>2.556181</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.449000</td>\n",
       "      <td>2.532333</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.416100</td>\n",
       "      <td>2.504129</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.394500</td>\n",
       "      <td>2.493205</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.356200</td>\n",
       "      <td>2.457393</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.334500</td>\n",
       "      <td>2.438286</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.288500</td>\n",
       "      <td>2.406151</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.258500</td>\n",
       "      <td>2.375241</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.233300</td>\n",
       "      <td>2.345837</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.205500</td>\n",
       "      <td>2.325907</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>2.321353</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.162700</td>\n",
       "      <td>2.305702</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.148400</td>\n",
       "      <td>2.288197</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.131500</td>\n",
       "      <td>2.269367</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.098800</td>\n",
       "      <td>2.250685</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.087500</td>\n",
       "      <td>2.227517</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>2.199621</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.030800</td>\n",
       "      <td>2.183851</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.007700</td>\n",
       "      <td>2.167749</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.007900</td>\n",
       "      <td>2.159802</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>2.974700</td>\n",
       "      <td>2.125615</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>2.945600</td>\n",
       "      <td>2.105867</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.929600</td>\n",
       "      <td>2.102233</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>2.909900</td>\n",
       "      <td>2.070078</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.888500</td>\n",
       "      <td>2.068535</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>2.891000</td>\n",
       "      <td>2.057243</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>2.854400</td>\n",
       "      <td>2.049853</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>2.838800</td>\n",
       "      <td>2.010796</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>2.812800</td>\n",
       "      <td>2.002504</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>2.809600</td>\n",
       "      <td>2.004140</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>2.804700</td>\n",
       "      <td>1.979581</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>2.779000</td>\n",
       "      <td>1.965669</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>2.760500</td>\n",
       "      <td>1.956525</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>2.752000</td>\n",
       "      <td>1.972878</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>2.752300</td>\n",
       "      <td>1.950397</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>2.741900</td>\n",
       "      <td>1.983582</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>2.740500</td>\n",
       "      <td>1.929517</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>2.714600</td>\n",
       "      <td>1.918194</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>2.705700</td>\n",
       "      <td>1.927531</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>2.689800</td>\n",
       "      <td>1.909100</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>2.695300</td>\n",
       "      <td>1.916104</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>2.679100</td>\n",
       "      <td>1.917845</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>2.659000</td>\n",
       "      <td>1.891651</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>2.652400</td>\n",
       "      <td>1.881567</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>2.647400</td>\n",
       "      <td>1.879860</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>2.635000</td>\n",
       "      <td>1.887548</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>2.631200</td>\n",
       "      <td>1.855459</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>2.619400</td>\n",
       "      <td>1.892967</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>2.622700</td>\n",
       "      <td>1.893683</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>2.616400</td>\n",
       "      <td>1.849980</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>2.592100</td>\n",
       "      <td>1.833120</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>2.600700</td>\n",
       "      <td>1.830926</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>2.587800</td>\n",
       "      <td>1.835634</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>2.586100</td>\n",
       "      <td>1.841103</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>2.569900</td>\n",
       "      <td>1.815884</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>2.569600</td>\n",
       "      <td>1.811804</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>2.551700</td>\n",
       "      <td>1.808834</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>2.557600</td>\n",
       "      <td>1.795377</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>2.540400</td>\n",
       "      <td>1.791092</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>2.546000</td>\n",
       "      <td>1.786948</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>2.547700</td>\n",
       "      <td>1.781433</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>2.532500</td>\n",
       "      <td>1.782383</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>2.523200</td>\n",
       "      <td>1.794116</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>2.535600</td>\n",
       "      <td>1.809228</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>2.532200</td>\n",
       "      <td>1.805050</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>2.516800</td>\n",
       "      <td>1.774886</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>2.506700</td>\n",
       "      <td>1.767230</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>2.512700</td>\n",
       "      <td>1.799715</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>2.532900</td>\n",
       "      <td>1.797630</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>2.512300</td>\n",
       "      <td>1.777654</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>2.493900</td>\n",
       "      <td>1.751276</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>2.478700</td>\n",
       "      <td>1.755382</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>2.474600</td>\n",
       "      <td>1.756062</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>2.471900</td>\n",
       "      <td>1.758009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>2.465600</td>\n",
       "      <td>1.746564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>2.469300</td>\n",
       "      <td>1.741552</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>2.462600</td>\n",
       "      <td>1.734575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>2.455600</td>\n",
       "      <td>1.764289</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>2.442100</td>\n",
       "      <td>1.729248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>2.441700</td>\n",
       "      <td>1.734075</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>2.444100</td>\n",
       "      <td>1.721476</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>2.430100</td>\n",
       "      <td>1.715119</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>2.434200</td>\n",
       "      <td>1.741067</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>2.426500</td>\n",
       "      <td>1.738196</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>2.438900</td>\n",
       "      <td>1.702990</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>2.425900</td>\n",
       "      <td>1.734967</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>2.416800</td>\n",
       "      <td>1.702700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>2.415500</td>\n",
       "      <td>1.721648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>2.428800</td>\n",
       "      <td>1.730291</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>2.424900</td>\n",
       "      <td>1.727375</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>2.436700</td>\n",
       "      <td>1.754814</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>2.430700</td>\n",
       "      <td>1.740140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>2.435300</td>\n",
       "      <td>1.709872</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>2.403900</td>\n",
       "      <td>1.727430</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>2.415700</td>\n",
       "      <td>1.718621</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>2.424200</td>\n",
       "      <td>1.735239</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>2.405900</td>\n",
       "      <td>1.783336</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>2.413500</td>\n",
       "      <td>1.693468</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>2.405100</td>\n",
       "      <td>1.717842</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>2.396500</td>\n",
       "      <td>1.681246</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>2.386700</td>\n",
       "      <td>1.684548</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>2.376400</td>\n",
       "      <td>1.676524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>2.374200</td>\n",
       "      <td>1.667825</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>2.375300</td>\n",
       "      <td>1.672514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>2.366100</td>\n",
       "      <td>1.678815</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>2.368500</td>\n",
       "      <td>1.664724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>2.371800</td>\n",
       "      <td>1.680066</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>2.350300</td>\n",
       "      <td>1.654941</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>2.361500</td>\n",
       "      <td>1.664055</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>2.361300</td>\n",
       "      <td>1.661851</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>2.364100</td>\n",
       "      <td>1.672773</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>2.357100</td>\n",
       "      <td>1.644008</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>2.354800</td>\n",
       "      <td>1.661373</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>2.363800</td>\n",
       "      <td>1.667967</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>2.353200</td>\n",
       "      <td>1.642680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>2.343700</td>\n",
       "      <td>1.663869</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>2.354800</td>\n",
       "      <td>1.665673</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>2.355500</td>\n",
       "      <td>1.679460</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>2.349300</td>\n",
       "      <td>1.656686</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>2.340200</td>\n",
       "      <td>1.657807</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>2.334000</td>\n",
       "      <td>1.640584</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>2.346800</td>\n",
       "      <td>1.640298</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>2.332200</td>\n",
       "      <td>1.637357</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>2.335600</td>\n",
       "      <td>1.631920</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>2.326900</td>\n",
       "      <td>1.634889</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>2.337500</td>\n",
       "      <td>1.675974</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>2.347200</td>\n",
       "      <td>1.649949</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>2.330500</td>\n",
       "      <td>1.629854</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>2.334100</td>\n",
       "      <td>1.629902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>2.330200</td>\n",
       "      <td>1.621953</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>2.323500</td>\n",
       "      <td>1.620871</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>2.318200</td>\n",
       "      <td>1.625480</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>2.324400</td>\n",
       "      <td>1.630661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>2.318100</td>\n",
       "      <td>1.634706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>2.324100</td>\n",
       "      <td>1.630057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>2.318800</td>\n",
       "      <td>1.625736</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>2.319100</td>\n",
       "      <td>1.625971</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>2.314100</td>\n",
       "      <td>1.618580</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>2.307100</td>\n",
       "      <td>1.620780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>2.318600</td>\n",
       "      <td>1.623411</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>2.309800</td>\n",
       "      <td>1.626697</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>2.315600</td>\n",
       "      <td>1.620647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>2.309200</td>\n",
       "      <td>1.618235</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>2.305300</td>\n",
       "      <td>1.617915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>2.305100</td>\n",
       "      <td>1.609588</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>2.302300</td>\n",
       "      <td>1.609322</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>2.301300</td>\n",
       "      <td>1.607570</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>2.306700</td>\n",
       "      <td>1.608582</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>2.301600</td>\n",
       "      <td>1.608264</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>2.305400</td>\n",
       "      <td>1.610508</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>2.301800</td>\n",
       "      <td>1.609683</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>2.299900</td>\n",
       "      <td>1.604631</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>2.297300</td>\n",
       "      <td>1.605814</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179000</td>\n",
       "      <td>2.299400</td>\n",
       "      <td>1.605089</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-2000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-2000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-20000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-4000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-4000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-22000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-6000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-6000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-24000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-8000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-8000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-2000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-10000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-10000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-4000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-12000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-12000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-6000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-14000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-14000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-8000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-16000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-16000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-10000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-18000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-18000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-12000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-20000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-20000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-14000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-22000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-22000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-16000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-24000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-24000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-18000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-26000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-26000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-20000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-28000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-28000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-22000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-30000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-30000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-24000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-32000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-32000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-26000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-34000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-34000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-28000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-36000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-36000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-30000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-38000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-38000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-32000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-40000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-40000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-34000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-42000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-42000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-36000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-44000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-44000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-38000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-46000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-46000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-40000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-48000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-48000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-42000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-50000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-50000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-44000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-52000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-52000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-46000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-54000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-54000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-48000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-56000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-56000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-50000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-58000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-58000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-52000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-60000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-60000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-54000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-62000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-62000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-56000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-64000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-64000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-58000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-66000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-66000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-60000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-68000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-68000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-62000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-70000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-70000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-64000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-72000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-72000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-72000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-66000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-74000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-74000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-74000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-68000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-76000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-76000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-76000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-70000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-78000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-78000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-78000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-72000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-80000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-80000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-74000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-82000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-82000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-82000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-76000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-84000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-84000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-84000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-78000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-86000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-86000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-86000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-80000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-88000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-88000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-88000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-82000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-90000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-90000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-84000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-92000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-92000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-92000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-86000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-94000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-94000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-94000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-88000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-96000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-96000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-96000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-90000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-98000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-98000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-98000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-92000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-100000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-100000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-100000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-94000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-102000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-102000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-102000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-96000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-104000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-104000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-104000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-98000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-106000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-106000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-106000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-100000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-108000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-108000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-108000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-102000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-110000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-110000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-110000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-104000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-112000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-112000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-112000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-106000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-114000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-114000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-114000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-108000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-116000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-116000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-116000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-110000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-118000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-118000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-118000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-112000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-120000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-120000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-120000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-114000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-122000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-122000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-122000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-116000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-124000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-124000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-124000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-118000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-126000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-126000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-126000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-120000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-128000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-128000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-128000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-122000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-130000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-130000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-130000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-124000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-132000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-132000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-132000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-126000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-134000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-134000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-134000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-128000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-136000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-136000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-136000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-130000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-138000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-138000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-138000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-132000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-140000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-140000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-140000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-134000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-142000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-142000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-142000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-136000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-144000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-144000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-144000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-138000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-146000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-146000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-146000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-140000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-148000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-148000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-148000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-142000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-150000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-150000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-150000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-144000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-152000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-152000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-152000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-146000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-154000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-154000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-154000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-148000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-156000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-156000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-156000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-150000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-158000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-158000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-158000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-152000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-160000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-160000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-160000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-154000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-162000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-162000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-162000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-156000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-164000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-164000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-164000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-158000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-166000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-166000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-166000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-160000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-168000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-168000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-168000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-162000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-170000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-170000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-170000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-164000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-172000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-172000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-172000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-166000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-174000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-174000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-174000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-168000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-176000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-176000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-176000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-170000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-178000\n",
      "Configuration saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-178000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint/roberta2gpt/checkpoint-178000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint/roberta2gpt/checkpoint-172000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=179500, training_loss=2.7927890720208044, metrics={'train_runtime': 210481.0711, 'train_samples_per_second': 13.641, 'train_steps_per_second': 0.853, 'total_flos': 8.805162054057984e+17, 'train_loss': 2.7927890720208044, 'epoch': 100.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    output_dir=os.path.join(checkpoint_dir, \"roberta2gpt\"),\n",
    "    # do_train=True,\n",
    "    # do_eval=True,\n",
    "    # do_predict=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=4,\n",
    "#     learning_rate=1e-4, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0,\n",
    "    num_train_epochs=100,\n",
    "    max_steps=-1,\n",
    "    # lr_scheduler_type='linear', warmup_ratio=0.0, \n",
    "    \n",
    "    logging_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=1000,\n",
    "    save_steps=2000,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=10000,\n",
    "    save_total_limit=3,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be7e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b0595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd93c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ac735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff11ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245a07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0da071a",
   "metadata": {},
   "source": [
    "https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1cd79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16/resolve/main/config.json from cache at /home/yoomin/.cache/huggingface/transformers/7944e7bc294d39091dcbf5fc5c6fc0d3d32cda1fc3e6208912c82482c489a888.2f0b414aab7a259e6c2cb673dc14da52f6b46ac536982d5c10f29c8fec3fadef\n",
      "You are using a model of type encoder_decoder to instantiate a model of type encoder-decoder. This is not supported for all configurations of models and can yield errors.\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"return_dict\": false,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"sep_token_id\": null,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "      \"text-generation\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_length\": 50\n",
      "      }\n",
      "    },\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.17.0\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": false,\n",
      "    \"vocab_size\": 50257\n",
      "  },\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": false,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.17.0\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 28996\n",
      "  },\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"transformers_version\": null\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16/resolve/main/pytorch_model.bin from cache at /home/yoomin/.cache/huggingface/transformers/1ac8f214e43fa2853babfd2021fdee5b7efcdd1ac408a0b76bda6354a994e869.6b6cdb7638725837ff9a411dfb044b2ce772b14f90a62ea06bfcf1d0be1c3f95\n",
      "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
      "\n",
      "All the weights of EncoderDecoderModel were initialized from the model checkpoint at patrickvonplaten/bert2gpt2-cnn_dailymail-fp16.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m EncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatrickvonplaten/bert2gpt2-cnn_dailymail-fp16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m      4\u001b[0m bert_tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# CLS token will work as BOS token\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2gpt2-cnn_dailymail-fp16\")\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# CLS token will work as BOS token\n",
    "bert_tokenizer.bos_token = bert_tokenizer.cls_token\n",
    "\n",
    "# SEP token will work as EOS token\n",
    "bert_tokenizer.eos_token = bert_tokenizer.sep_token\n",
    "\n",
    "\n",
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
    "\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = gpt2_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = gpt2_tokenizer.eos_token_id\n",
    "model.config.max_length = 142\n",
    "model.config.min_length = 56\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4\n",
    "\n",
    "test_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = bert_tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "results = test_dataset.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bac2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488e8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86b46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
