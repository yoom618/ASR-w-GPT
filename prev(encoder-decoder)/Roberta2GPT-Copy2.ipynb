{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39678a37",
   "metadata": {},
   "source": [
    "### 0. Initial Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0bf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install datasets==1.0.2\n",
    "# !pip install transformers==4.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44238cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoom-private\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yoomin/wandb/run-20220405_123838-1tsrbzcv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yoom-private/testing-roberta2gpt/runs/1tsrbzcv\" target=\"_blank\">skilled-plasma-2</a></strong> to <a href=\"https://wandb.ai/yoom-private/testing-roberta2gpt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/yoom-private/testing-roberta2gpt/runs/1tsrbzcv?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f755b23a070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "cache_dir = \"/data4/yoomcache\"\n",
    "model_cache_dir = os.path.join(cache_dir, 'huggingface')\n",
    "data_cache_dir = os.path.join(cache_dir, 'datasets')\n",
    "checkpoint_dir = os.path.join(cache_dir, 'checkpoint2')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "from transformers import BertTokenizer, RobertaTokenizer, GPT2Tokenizer\n",
    "from transformers import AutoConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"testing-roberta2gpt\", entity=\"yoom-private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6e87b",
   "metadata": {},
   "source": [
    "### 1. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435b40c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_encoder = AutoConfig.from_pretrained(\"roberta-base\", cache_dir=model_cache_dir)\n",
    "config_decoder = AutoConfig.from_pretrained(\"gpt2\", cache_dir=model_cache_dir)\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder, cache_dir=model_cache_dir)\n",
    "model = EncoderDecoderModel(config=config)\n",
    "# model.save_pretrained(\"roberta2gpt\", cache_dir=model_cache_dir)\n",
    "# model = EncoderDecoderModel.from_pretrained(\"roberta2gpt\", cache_dir=model_cache_dir)\n",
    "\n",
    "model.encoder.encoder.layer = model.encoder.encoder.layer[:8]\n",
    "model.decoder.transformer.h = model.decoder.transformer.h[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe24aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", cache_dir=model_cache_dir)\n",
    "encoder_tokenizer.bos_token = encoder_tokenizer.cls_token  # CLS token will work as BOS token\n",
    "encoder_tokenizer.eos_token = encoder_tokenizer.sep_token  # SEP token will work as EOS token\n",
    "\n",
    "# make sure GPT2 appends EOS in begin and end\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "decoder_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=model_cache_dir)\n",
    "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.unk_token\n",
    "\n",
    "\n",
    "model.config.decoder_start_token_id = encoder_tokenizer.cls_token_id\n",
    "model.config.eos_token_id = encoder_tokenizer.sep_token_id\n",
    "model.config.pad_token_id = encoder_tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.max_length = 142\n",
    "model.config.min_length = 56\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze encoder parameters\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "with torch.no_grad():\n",
    "    model.encoder\n",
    "    \n",
    "# # Freeze decoder parameters\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bce84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467b21e7",
   "metadata": {},
   "source": [
    "### 2. Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feaded22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):    # Tokenizer will automatically set [BOS] <text> [EOS] \n",
    "    encoder_length, decoder_length = 512, 128\n",
    "    inputs = encoder_tokenizer(batch[\"article\"], \n",
    "                               padding=\"max_length\", \n",
    "                               truncation=True, \n",
    "                               max_length=encoder_length)\n",
    "    outputs = decoder_tokenizer(batch[\"highlights\"], \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                max_length=decoder_length)\n",
    "    \n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "\n",
    "    # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "    batch[\"labels\"] = -100 if batch[\"decoder_attention_mask\"] == 0 else batch[\"labels\"]\n",
    "\n",
    "    assert len(inputs.input_ids) == encoder_length\n",
    "    assert len(outputs.input_ids) == decoder_length\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d77189b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(cache_dir, 'preprocessed/train')):\n",
    "    train_dataset = load_from_disk(os.path.join(cache_dir, 'preprocessed/train'))\n",
    "else:\n",
    "    train_dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\", split=\"train\", cache_dir=data_cache_dir)\n",
    "    train_dataset = train_dataset.map(\n",
    "        map_to_encoder_decoder_inputs, \n",
    "        # batched=True, \n",
    "        # batch_size=batch_size, \n",
    "        remove_columns=['id', 'article', 'highlights'],\n",
    "    )\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", \n",
    "        columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    "    )\n",
    "    \n",
    "    train_dataset.save_to_disk(os.path.join(cache_dir, 'preprocessed/train'))\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(cache_dir, 'preprocessed/val')):\n",
    "    val_dataset = load_from_disk(os.path.join(cache_dir, 'preprocessed/val'))\n",
    "else:\n",
    "    val_dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\", split=\"validation\", cache_dir=data_cache_dir)\n",
    "    val_dataset = val_dataset.map(\n",
    "        map_to_encoder_decoder_inputs, \n",
    "        # batched=True, \n",
    "        # batch_size=batch_size, \n",
    "        remove_columns=['id', 'article', 'highlights'],\n",
    "    )\n",
    "    val_dataset.set_format(\n",
    "        type=\"torch\", \n",
    "        columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    "    )\n",
    "    val_dataset.save_to_disk(os.path.join(cache_dir, 'preprocessed/val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80414621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63779bf8",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cde9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = load_metric(\"rouge\")\n",
    "# rouge = load_metric(\"rouge\", experiment_id=1)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "    label_str = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453320d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 28711\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 179500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68050' max='179500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 68050/179500 19:44:10 < 32:19:27, 0.96 it/s, Epoch 37.91/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.468400</td>\n",
       "      <td>0.363385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.483969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.012569</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008349</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.015623</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.017890</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015384</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016882</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017018</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020787</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-1000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-1000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-88000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-2000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-2000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-89000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-3000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-3000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-90000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-4000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-4000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-91000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-5000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-5000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-92000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-6000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-6000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-93000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-7000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-7000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-94000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-8000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-8000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-95000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-9000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-9000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-96000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-10000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-10000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-97000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-11000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-11000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-1000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-12000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-12000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-2000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-13000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-13000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-3000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-14000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-14000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-4000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-15000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-15000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-5000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-16000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-16000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-6000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-17000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-17000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-7000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-18000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-18000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-8000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-19000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-19000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-9000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-20000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-20000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-10000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-21000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-21000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-11000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-22000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-22000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-12000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-23000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-23000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-13000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-24000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-24000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-14000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-25000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-25000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-15000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-26000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-26000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-16000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-27000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-27000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-17000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-28000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-28000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-18000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-29000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-29000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-19000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-30000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-30000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-20000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-31000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-31000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-21000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-32000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-32000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-22000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-33000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-33000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-23000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-34000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-34000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-24000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-35000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-35000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-25000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-36000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-36000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-26000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-37000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-37000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-27000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-38000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-38000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-28000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-39000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-39000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-29000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-40000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-40000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-30000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-41000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-41000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-31000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-42000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-42000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-32000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-43000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-43000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-33000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-44000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-44000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-34000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-45000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-45000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-35000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-46000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-46000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-36000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-47000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-47000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-37000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-48000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-48000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-38000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-49000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-49000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-39000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-50000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-50000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-40000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-51000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-51000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-41000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-52000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-52000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-42000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-53000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-53000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-43000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-54000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-54000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-44000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-55000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-55000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-45000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-56000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-56000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-46000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-57000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-57000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-47000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-58000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-58000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-48000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-59000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-59000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-49000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-60000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-60000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-50000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-61000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-61000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-51000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-62000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-62000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-52000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-63000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-63000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-53000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-64000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-64000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-54000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-65000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-65000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-55000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-66000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-66000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-56000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-67000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-67000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-67000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-57000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1337\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-68000\n",
      "Configuration saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-68000/config.json\n",
      "Model weights saved in /data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data4/yoomcache/checkpoint2/roberta2gpt/checkpoint-58000] due to args.save_total_limit\n",
      "/home/yoomin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:531: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    output_dir=os.path.join(checkpoint_dir, \"roberta2gpt\"),\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # do_predict=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-4, \n",
    "#     weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0,\n",
    "    num_train_epochs=100,\n",
    "    max_steps=-1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    \n",
    "    logging_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=10000,\n",
    "    save_total_limit=10,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be7e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b0595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd93c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ac735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff11ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245a07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488e8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86b46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
